{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "Trivial case of fitting a straight line \n",
    "\n",
    "Note: we use $t$ here for time, but it is the same as using $(x,y)$.\n",
    "\n",
    "a. We have $N$ data points with input {$t_n$} and targets {$x_n$} \n",
    "\n",
    "b. Want to find the best fit line $f_n=k_1t_n+k_0$ in the least-squares sense\n",
    "\n",
    "c. Define an error function $E=\\frac{1}{2N}\\sum_{n=1}^{N}(f_n-x_n)^2$\n",
    "\n",
    "d. Goal: Find $k_0, k_1$ to minimize $E$\n",
    "\n",
    "### 1.1 Using calculus, we have to minimizse $\\frac{\\partial E}{\\partial k_i}=0$:\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$k_0 = \\frac{\\langle t^2\\rangle \\langle x\\rangle -\\langle xt\\rangle \\langle t\\rangle }{\\langle t^2\\rangle -\\langle t\\rangle ^2}$\n",
    "\n",
    "$k_1= \\frac{\\langle xt\\rangle - \\langle x\\rangle \\langle t\\rangle}{\\langle t^2\\rangle - \\langle t\\rangle ^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Example fitting x = 3t-5\n",
    "\n",
    "$k_0 =-5$,\n",
    "$k_1=3$\n",
    "\n",
    "Notice that increasing $N$ makes the estimates of $k_0,k_1$ closer to the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N=100\n",
    "mu,sigma=0,1\n",
    "e=np.random.normal(mu,sigma,N)\n",
    "\n",
    "T=np.linspace(0,10,N)\n",
    "X=3*T-5+e\n",
    "\n",
    "plt.plot(T,X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean=np.mean(T)\n",
    "t2_mean=np.mean(T**2)\n",
    "x_mean=np.mean(X)\n",
    "xt_mean=np.mean(X*T)\n",
    "\n",
    "k_0=(t2_mean*x_mean-xt_mean*t_mean)/(t2_mean-t_mean**2)\n",
    "k_1=(xt_mean-x_mean*t_mean)/(t2_mean-t_mean**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_0= -5.07364509615009  k_1= 3.013563613057548\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "F=T*k_1+k_0\n",
    "print('k_0=',k_0,' k_1=',k_1)\n",
    "\n",
    "plt.plot(T,X,label='Actual',lw=2,color='gold')\n",
    "plt.plot(T,F,label='Model',lw=2)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using PyTorch to fit this automatically\n",
    "\n",
    "#### 1.3.1 Model Architecture\n",
    "\n",
    "Define model architecture through a class that takes torch.nn.Module, the basic neural network module containing the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self,inputSize,outputSize):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear=torch.nn.Linear(inputSize,outputSize)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Model Instantiation\n",
    "\n",
    "Instantiate the model, initialize the loss and optimization functions for training the model. We are using the mean squared error loss function and stochastic gradient descent for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim=1        \n",
    "outputDim=1   \n",
    "learningRate=0.01 \n",
    "epochs=1000\n",
    "\n",
    "model=LinearRegression(inputDim,outputDim)\n",
    "\n",
    "# For GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.MSELoss() \n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Model Training\n",
    "\n",
    "Increasing the number of epochs will get the results to greater accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(82.1016, grad_fn=<MseLossBackward>)\n",
      "epoch 0,loss 82.10160064697266\n",
      "tensor(17.9285, grad_fn=<MseLossBackward>)\n",
      "epoch 1,loss 17.928539276123047\n",
      "tensor(11.4773, grad_fn=<MseLossBackward>)\n",
      "epoch 2,loss 11.477286338806152\n",
      "tensor(10.7518, grad_fn=<MseLossBackward>)\n",
      "epoch 3,loss 10.751809120178223\n",
      "tensor(10.5951, grad_fn=<MseLossBackward>)\n",
      "epoch 4,loss 10.595067977905273\n",
      "tensor(10.4956, grad_fn=<MseLossBackward>)\n",
      "epoch 5,loss 10.495574951171875\n",
      "tensor(10.4026, grad_fn=<MseLossBackward>)\n",
      "epoch 6,loss 10.40259075164795\n",
      "tensor(10.3111, grad_fn=<MseLossBackward>)\n",
      "epoch 7,loss 10.311077117919922\n",
      "tensor(10.2205, grad_fn=<MseLossBackward>)\n",
      "epoch 8,loss 10.2205228805542\n",
      "tensor(10.1309, grad_fn=<MseLossBackward>)\n",
      "epoch 9,loss 10.13086986541748\n",
      "tensor(10.0421, grad_fn=<MseLossBackward>)\n",
      "epoch 10,loss 10.042106628417969\n",
      "tensor(9.9542, grad_fn=<MseLossBackward>)\n",
      "epoch 11,loss 9.954219818115234\n",
      "tensor(9.8672, grad_fn=<MseLossBackward>)\n",
      "epoch 12,loss 9.867205619812012\n",
      "tensor(9.7811, grad_fn=<MseLossBackward>)\n",
      "epoch 13,loss 9.781054496765137\n",
      "tensor(9.6958, grad_fn=<MseLossBackward>)\n",
      "epoch 14,loss 9.695755004882812\n",
      "tensor(9.6113, grad_fn=<MseLossBackward>)\n",
      "epoch 15,loss 9.611300468444824\n",
      "tensor(9.5277, grad_fn=<MseLossBackward>)\n",
      "epoch 16,loss 9.527682304382324\n",
      "tensor(9.4449, grad_fn=<MseLossBackward>)\n",
      "epoch 17,loss 9.444890975952148\n",
      "tensor(9.3629, grad_fn=<MseLossBackward>)\n",
      "epoch 18,loss 9.362918853759766\n",
      "tensor(9.2818, grad_fn=<MseLossBackward>)\n",
      "epoch 19,loss 9.281763076782227\n",
      "tensor(9.2014, grad_fn=<MseLossBackward>)\n",
      "epoch 20,loss 9.201408386230469\n",
      "tensor(9.1218, grad_fn=<MseLossBackward>)\n",
      "epoch 21,loss 9.121849060058594\n",
      "tensor(9.0431, grad_fn=<MseLossBackward>)\n",
      "epoch 22,loss 9.043076515197754\n",
      "tensor(8.9651, grad_fn=<MseLossBackward>)\n",
      "epoch 23,loss 8.96508502960205\n",
      "tensor(8.8879, grad_fn=<MseLossBackward>)\n",
      "epoch 24,loss 8.88786792755127\n",
      "tensor(8.8114, grad_fn=<MseLossBackward>)\n",
      "epoch 25,loss 8.811412811279297\n",
      "tensor(8.7357, grad_fn=<MseLossBackward>)\n",
      "epoch 26,loss 8.735715866088867\n",
      "tensor(8.6608, grad_fn=<MseLossBackward>)\n",
      "epoch 27,loss 8.66076946258545\n",
      "tensor(8.5866, grad_fn=<MseLossBackward>)\n",
      "epoch 28,loss 8.586564064025879\n",
      "tensor(8.5131, grad_fn=<MseLossBackward>)\n",
      "epoch 29,loss 8.513093948364258\n",
      "tensor(8.4404, grad_fn=<MseLossBackward>)\n",
      "epoch 30,loss 8.440352439880371\n",
      "tensor(8.3683, grad_fn=<MseLossBackward>)\n",
      "epoch 31,loss 8.368330001831055\n",
      "tensor(8.2970, grad_fn=<MseLossBackward>)\n",
      "epoch 32,loss 8.297021865844727\n",
      "tensor(8.2264, grad_fn=<MseLossBackward>)\n",
      "epoch 33,loss 8.226418495178223\n",
      "tensor(8.1565, grad_fn=<MseLossBackward>)\n",
      "epoch 34,loss 8.156515121459961\n",
      "tensor(8.0873, grad_fn=<MseLossBackward>)\n",
      "epoch 35,loss 8.087303161621094\n",
      "tensor(8.0188, grad_fn=<MseLossBackward>)\n",
      "epoch 36,loss 8.018777847290039\n",
      "tensor(7.9509, grad_fn=<MseLossBackward>)\n",
      "epoch 37,loss 7.950930595397949\n",
      "tensor(7.8838, grad_fn=<MseLossBackward>)\n",
      "epoch 38,loss 7.883756637573242\n",
      "tensor(7.8172, grad_fn=<MseLossBackward>)\n",
      "epoch 39,loss 7.817245960235596\n",
      "tensor(7.7514, grad_fn=<MseLossBackward>)\n",
      "epoch 40,loss 7.7513933181762695\n",
      "tensor(7.6862, grad_fn=<MseLossBackward>)\n",
      "epoch 41,loss 7.686194896697998\n",
      "tensor(7.6216, grad_fn=<MseLossBackward>)\n",
      "epoch 42,loss 7.621641635894775\n",
      "tensor(7.5577, grad_fn=<MseLossBackward>)\n",
      "epoch 43,loss 7.557728290557861\n",
      "tensor(7.4944, grad_fn=<MseLossBackward>)\n",
      "epoch 44,loss 7.494446277618408\n",
      "tensor(7.4318, grad_fn=<MseLossBackward>)\n",
      "epoch 45,loss 7.43179178237915\n",
      "tensor(7.3698, grad_fn=<MseLossBackward>)\n",
      "epoch 46,loss 7.369758129119873\n",
      "tensor(7.3083, grad_fn=<MseLossBackward>)\n",
      "epoch 47,loss 7.308338642120361\n",
      "tensor(7.2475, grad_fn=<MseLossBackward>)\n",
      "epoch 48,loss 7.247527599334717\n",
      "tensor(7.1873, grad_fn=<MseLossBackward>)\n",
      "epoch 49,loss 7.187319278717041\n",
      "tensor(7.1277, grad_fn=<MseLossBackward>)\n",
      "epoch 50,loss 7.127705097198486\n",
      "tensor(7.0687, grad_fn=<MseLossBackward>)\n",
      "epoch 51,loss 7.0686821937561035\n",
      "tensor(7.0102, grad_fn=<MseLossBackward>)\n",
      "epoch 52,loss 7.010244846343994\n",
      "tensor(6.9524, grad_fn=<MseLossBackward>)\n",
      "epoch 53,loss 6.952385902404785\n",
      "tensor(6.8951, grad_fn=<MseLossBackward>)\n",
      "epoch 54,loss 6.895100116729736\n",
      "tensor(6.8384, grad_fn=<MseLossBackward>)\n",
      "epoch 55,loss 6.838381290435791\n",
      "tensor(6.7822, grad_fn=<MseLossBackward>)\n",
      "epoch 56,loss 6.782224655151367\n",
      "tensor(6.7266, grad_fn=<MseLossBackward>)\n",
      "epoch 57,loss 6.726622104644775\n",
      "tensor(6.6716, grad_fn=<MseLossBackward>)\n",
      "epoch 58,loss 6.671572208404541\n",
      "tensor(6.6171, grad_fn=<MseLossBackward>)\n",
      "epoch 59,loss 6.617065906524658\n",
      "tensor(6.5631, grad_fn=<MseLossBackward>)\n",
      "epoch 60,loss 6.563100814819336\n",
      "tensor(6.5097, grad_fn=<MseLossBackward>)\n",
      "epoch 61,loss 6.509669303894043\n",
      "tensor(6.4568, grad_fn=<MseLossBackward>)\n",
      "epoch 62,loss 6.456767559051514\n",
      "tensor(6.4044, grad_fn=<MseLossBackward>)\n",
      "epoch 63,loss 6.404390811920166\n",
      "tensor(6.3525, grad_fn=<MseLossBackward>)\n",
      "epoch 64,loss 6.352530479431152\n",
      "tensor(6.3012, grad_fn=<MseLossBackward>)\n",
      "epoch 65,loss 6.30118465423584\n",
      "tensor(6.2503, grad_fn=<MseLossBackward>)\n",
      "epoch 66,loss 6.250346660614014\n",
      "tensor(6.2000, grad_fn=<MseLossBackward>)\n",
      "epoch 67,loss 6.200014114379883\n",
      "tensor(6.1502, grad_fn=<MseLossBackward>)\n",
      "epoch 68,loss 6.150178909301758\n",
      "tensor(6.1008, grad_fn=<MseLossBackward>)\n",
      "epoch 69,loss 6.100836753845215\n",
      "tensor(6.0520, grad_fn=<MseLossBackward>)\n",
      "epoch 70,loss 6.051984786987305\n",
      "tensor(6.0036, grad_fn=<MseLossBackward>)\n",
      "epoch 71,loss 6.0036139488220215\n",
      "tensor(5.9557, grad_fn=<MseLossBackward>)\n",
      "epoch 72,loss 5.955724716186523\n",
      "tensor(5.9083, grad_fn=<MseLossBackward>)\n",
      "epoch 73,loss 5.908308029174805\n",
      "tensor(5.8614, grad_fn=<MseLossBackward>)\n",
      "epoch 74,loss 5.861361026763916\n",
      "tensor(5.8149, grad_fn=<MseLossBackward>)\n",
      "epoch 75,loss 5.81488037109375\n",
      "tensor(5.7689, grad_fn=<MseLossBackward>)\n",
      "epoch 76,loss 5.768858432769775\n",
      "tensor(5.7233, grad_fn=<MseLossBackward>)\n",
      "epoch 77,loss 5.723293304443359\n",
      "tensor(5.6782, grad_fn=<MseLossBackward>)\n",
      "epoch 78,loss 5.6781792640686035\n",
      "tensor(5.6335, grad_fn=<MseLossBackward>)\n",
      "epoch 79,loss 5.633511543273926\n",
      "tensor(5.5893, grad_fn=<MseLossBackward>)\n",
      "epoch 80,loss 5.589287757873535\n",
      "tensor(5.5455, grad_fn=<MseLossBackward>)\n",
      "epoch 81,loss 5.545499801635742\n",
      "tensor(5.5021, grad_fn=<MseLossBackward>)\n",
      "epoch 82,loss 5.5021467208862305\n",
      "tensor(5.4592, grad_fn=<MseLossBackward>)\n",
      "epoch 83,loss 5.459221363067627\n",
      "tensor(5.4167, grad_fn=<MseLossBackward>)\n",
      "epoch 84,loss 5.416723251342773\n",
      "tensor(5.3746, grad_fn=<MseLossBackward>)\n",
      "epoch 85,loss 5.374644756317139\n",
      "tensor(5.3330, grad_fn=<MseLossBackward>)\n",
      "epoch 86,loss 5.33298397064209\n",
      "tensor(5.2917, grad_fn=<MseLossBackward>)\n",
      "epoch 87,loss 5.2917351722717285\n",
      "tensor(5.2509, grad_fn=<MseLossBackward>)\n",
      "epoch 88,loss 5.250894069671631\n",
      "tensor(5.2105, grad_fn=<MseLossBackward>)\n",
      "epoch 89,loss 5.210458278656006\n",
      "tensor(5.1704, grad_fn=<MseLossBackward>)\n",
      "epoch 90,loss 5.17042350769043\n",
      "tensor(5.1308, grad_fn=<MseLossBackward>)\n",
      "epoch 91,loss 5.13078498840332\n",
      "tensor(5.0915, grad_fn=<MseLossBackward>)\n",
      "epoch 92,loss 5.091536998748779\n",
      "tensor(5.0527, grad_fn=<MseLossBackward>)\n",
      "epoch 93,loss 5.052680492401123\n",
      "tensor(5.0142, grad_fn=<MseLossBackward>)\n",
      "epoch 94,loss 5.014207363128662\n",
      "tensor(4.9761, grad_fn=<MseLossBackward>)\n",
      "epoch 95,loss 4.976116180419922\n",
      "tensor(4.9384, grad_fn=<MseLossBackward>)\n",
      "epoch 96,loss 4.938401699066162\n",
      "tensor(4.9011, grad_fn=<MseLossBackward>)\n",
      "epoch 97,loss 4.901060104370117\n",
      "tensor(4.8641, grad_fn=<MseLossBackward>)\n",
      "epoch 98,loss 4.86408805847168\n",
      "tensor(4.8275, grad_fn=<MseLossBackward>)\n",
      "epoch 99,loss 4.8274827003479\n",
      "tensor(4.7912, grad_fn=<MseLossBackward>)\n",
      "epoch 100,loss 4.791240215301514\n",
      "tensor(4.7554, grad_fn=<MseLossBackward>)\n",
      "epoch 101,loss 4.755357265472412\n",
      "tensor(4.7198, grad_fn=<MseLossBackward>)\n",
      "epoch 102,loss 4.7198286056518555\n",
      "tensor(4.6847, grad_fn=<MseLossBackward>)\n",
      "epoch 103,loss 4.684651851654053\n",
      "tensor(4.6498, grad_fn=<MseLossBackward>)\n",
      "epoch 104,loss 4.649824619293213\n",
      "tensor(4.6153, grad_fn=<MseLossBackward>)\n",
      "epoch 105,loss 4.615340709686279\n",
      "tensor(4.5812, grad_fn=<MseLossBackward>)\n",
      "epoch 106,loss 4.581198215484619\n",
      "tensor(4.5474, grad_fn=<MseLossBackward>)\n",
      "epoch 107,loss 4.547395706176758\n",
      "tensor(4.5139, grad_fn=<MseLossBackward>)\n",
      "epoch 108,loss 4.5139265060424805\n",
      "tensor(4.4808, grad_fn=<MseLossBackward>)\n",
      "epoch 109,loss 4.4807891845703125\n",
      "tensor(4.4480, grad_fn=<MseLossBackward>)\n",
      "epoch 110,loss 4.447980880737305\n",
      "tensor(4.4155, grad_fn=<MseLossBackward>)\n",
      "epoch 111,loss 4.415495872497559\n",
      "tensor(4.3833, grad_fn=<MseLossBackward>)\n",
      "epoch 112,loss 4.383333206176758\n",
      "tensor(4.3515, grad_fn=<MseLossBackward>)\n",
      "epoch 113,loss 4.351489067077637\n",
      "tensor(4.3200, grad_fn=<MseLossBackward>)\n",
      "epoch 114,loss 4.319960594177246\n",
      "tensor(4.2887, grad_fn=<MseLossBackward>)\n",
      "epoch 115,loss 4.28874397277832\n",
      "tensor(4.2578, grad_fn=<MseLossBackward>)\n",
      "epoch 116,loss 4.257836818695068\n",
      "tensor(4.2272, grad_fn=<MseLossBackward>)\n",
      "epoch 117,loss 4.227235794067383\n",
      "tensor(4.1969, grad_fn=<MseLossBackward>)\n",
      "epoch 118,loss 4.196937084197998\n",
      "tensor(4.1669, grad_fn=<MseLossBackward>)\n",
      "epoch 119,loss 4.166938304901123\n",
      "tensor(4.1372, grad_fn=<MseLossBackward>)\n",
      "epoch 120,loss 4.137236595153809\n",
      "tensor(4.1078, grad_fn=<MseLossBackward>)\n",
      "epoch 121,loss 4.107830047607422\n",
      "tensor(4.0787, grad_fn=<MseLossBackward>)\n",
      "epoch 122,loss 4.078714847564697\n",
      "tensor(4.0499, grad_fn=<MseLossBackward>)\n",
      "epoch 123,loss 4.049888610839844\n",
      "tensor(4.0213, grad_fn=<MseLossBackward>)\n",
      "epoch 124,loss 4.021345138549805\n",
      "tensor(3.9931, grad_fn=<MseLossBackward>)\n",
      "epoch 125,loss 3.993086576461792\n",
      "tensor(3.9651, grad_fn=<MseLossBackward>)\n",
      "epoch 126,loss 3.9651076793670654\n",
      "tensor(3.9374, grad_fn=<MseLossBackward>)\n",
      "epoch 127,loss 3.9374048709869385\n",
      "tensor(3.9100, grad_fn=<MseLossBackward>)\n",
      "epoch 128,loss 3.9099767208099365\n",
      "tensor(3.8828, grad_fn=<MseLossBackward>)\n",
      "epoch 129,loss 3.882819890975952\n",
      "tensor(3.8559, grad_fn=<MseLossBackward>)\n",
      "epoch 130,loss 3.8559319972991943\n",
      "tensor(3.8293, grad_fn=<MseLossBackward>)\n",
      "epoch 131,loss 3.8293120861053467\n",
      "tensor(3.8030, grad_fn=<MseLossBackward>)\n",
      "epoch 132,loss 3.8029544353485107\n",
      "tensor(3.7769, grad_fn=<MseLossBackward>)\n",
      "epoch 133,loss 3.7768585681915283\n",
      "tensor(3.7510, grad_fn=<MseLossBackward>)\n",
      "epoch 134,loss 3.7510199546813965\n",
      "tensor(3.7254, grad_fn=<MseLossBackward>)\n",
      "epoch 135,loss 3.7254385948181152\n",
      "tensor(3.7001, grad_fn=<MseLossBackward>)\n",
      "epoch 136,loss 3.7001092433929443\n",
      "tensor(3.6750, grad_fn=<MseLossBackward>)\n",
      "epoch 137,loss 3.675032138824463\n",
      "tensor(3.6502, grad_fn=<MseLossBackward>)\n",
      "epoch 138,loss 3.6502010822296143\n",
      "tensor(3.6256, grad_fn=<MseLossBackward>)\n",
      "epoch 139,loss 3.6256182193756104\n",
      "tensor(3.6013, grad_fn=<MseLossBackward>)\n",
      "epoch 140,loss 3.6012778282165527\n",
      "tensor(3.5772, grad_fn=<MseLossBackward>)\n",
      "epoch 141,loss 3.577178716659546\n",
      "tensor(3.5533, grad_fn=<MseLossBackward>)\n",
      "epoch 142,loss 3.5533177852630615\n",
      "tensor(3.5297, grad_fn=<MseLossBackward>)\n",
      "epoch 143,loss 3.529693365097046\n",
      "tensor(3.5063, grad_fn=<MseLossBackward>)\n",
      "epoch 144,loss 3.5063037872314453\n",
      "tensor(3.4831, grad_fn=<MseLossBackward>)\n",
      "epoch 145,loss 3.483144521713257\n",
      "tensor(3.4602, grad_fn=<MseLossBackward>)\n",
      "epoch 146,loss 3.4602155685424805\n",
      "tensor(3.4375, grad_fn=<MseLossBackward>)\n",
      "epoch 147,loss 3.4375131130218506\n",
      "tensor(3.4150, grad_fn=<MseLossBackward>)\n",
      "epoch 148,loss 3.415036201477051\n",
      "tensor(3.3928, grad_fn=<MseLossBackward>)\n",
      "epoch 149,loss 3.3927817344665527\n",
      "tensor(3.3707, grad_fn=<MseLossBackward>)\n",
      "epoch 150,loss 3.370745897293091\n",
      "tensor(3.3489, grad_fn=<MseLossBackward>)\n",
      "epoch 151,loss 3.3489315509796143\n",
      "tensor(3.3273, grad_fn=<MseLossBackward>)\n",
      "epoch 152,loss 3.3273305892944336\n",
      "tensor(3.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 153,loss 3.3059444427490234\n",
      "tensor(3.2848, grad_fn=<MseLossBackward>)\n",
      "epoch 154,loss 3.2847700119018555\n",
      "tensor(3.2638, grad_fn=<MseLossBackward>)\n",
      "epoch 155,loss 3.2638051509857178\n",
      "tensor(3.2430, grad_fn=<MseLossBackward>)\n",
      "epoch 156,loss 3.2430477142333984\n",
      "tensor(3.2225, grad_fn=<MseLossBackward>)\n",
      "epoch 157,loss 3.222496271133423\n",
      "tensor(3.2021, grad_fn=<MseLossBackward>)\n",
      "epoch 158,loss 3.2021491527557373\n",
      "tensor(3.1820, grad_fn=<MseLossBackward>)\n",
      "epoch 159,loss 3.182002067565918\n",
      "tensor(3.1621, grad_fn=<MseLossBackward>)\n",
      "epoch 160,loss 3.1620545387268066\n",
      "tensor(3.1423, grad_fn=<MseLossBackward>)\n",
      "epoch 161,loss 3.1423048973083496\n",
      "tensor(3.1228, grad_fn=<MseLossBackward>)\n",
      "epoch 162,loss 3.122751474380493\n",
      "tensor(3.1034, grad_fn=<MseLossBackward>)\n",
      "epoch 163,loss 3.103391170501709\n",
      "tensor(3.0842, grad_fn=<MseLossBackward>)\n",
      "epoch 164,loss 3.0842227935791016\n",
      "tensor(3.0652, grad_fn=<MseLossBackward>)\n",
      "epoch 165,loss 3.065244674682617\n",
      "tensor(3.0465, grad_fn=<MseLossBackward>)\n",
      "epoch 166,loss 3.0464534759521484\n",
      "tensor(3.0278, grad_fn=<MseLossBackward>)\n",
      "epoch 167,loss 3.0278491973876953\n",
      "tensor(3.0094, grad_fn=<MseLossBackward>)\n",
      "epoch 168,loss 3.0094285011291504\n",
      "tensor(2.9912, grad_fn=<MseLossBackward>)\n",
      "epoch 169,loss 2.9911913871765137\n",
      "tensor(2.9731, grad_fn=<MseLossBackward>)\n",
      "epoch 170,loss 2.9731333255767822\n",
      "tensor(2.9553, grad_fn=<MseLossBackward>)\n",
      "epoch 171,loss 2.955254554748535\n",
      "tensor(2.9376, grad_fn=<MseLossBackward>)\n",
      "epoch 172,loss 2.9375534057617188\n",
      "tensor(2.9200, grad_fn=<MseLossBackward>)\n",
      "epoch 173,loss 2.920027732849121\n",
      "tensor(2.9027, grad_fn=<MseLossBackward>)\n",
      "epoch 174,loss 2.9026741981506348\n",
      "tensor(2.8855, grad_fn=<MseLossBackward>)\n",
      "epoch 175,loss 2.885493755340576\n",
      "tensor(2.8685, grad_fn=<MseLossBackward>)\n",
      "epoch 176,loss 2.868483781814575\n",
      "tensor(2.8516, grad_fn=<MseLossBackward>)\n",
      "epoch 177,loss 2.851641893386841\n",
      "tensor(2.8350, grad_fn=<MseLossBackward>)\n",
      "epoch 178,loss 2.8349664211273193\n",
      "tensor(2.8185, grad_fn=<MseLossBackward>)\n",
      "epoch 179,loss 2.8184561729431152\n",
      "tensor(2.8021, grad_fn=<MseLossBackward>)\n",
      "epoch 180,loss 2.802109479904175\n",
      "tensor(2.7859, grad_fn=<MseLossBackward>)\n",
      "epoch 181,loss 2.7859249114990234\n",
      "tensor(2.7699, grad_fn=<MseLossBackward>)\n",
      "epoch 182,loss 2.769900321960449\n",
      "tensor(2.7540, grad_fn=<MseLossBackward>)\n",
      "epoch 183,loss 2.7540347576141357\n",
      "tensor(2.7383, grad_fn=<MseLossBackward>)\n",
      "epoch 184,loss 2.738326072692871\n",
      "tensor(2.7228, grad_fn=<MseLossBackward>)\n",
      "epoch 185,loss 2.7227728366851807\n",
      "tensor(2.7074, grad_fn=<MseLossBackward>)\n",
      "epoch 186,loss 2.707373857498169\n",
      "tensor(2.6921, grad_fn=<MseLossBackward>)\n",
      "epoch 187,loss 2.692126750946045\n",
      "tensor(2.6770, grad_fn=<MseLossBackward>)\n",
      "epoch 188,loss 2.6770315170288086\n",
      "tensor(2.6621, grad_fn=<MseLossBackward>)\n",
      "epoch 189,loss 2.6620852947235107\n",
      "tensor(2.6473, grad_fn=<MseLossBackward>)\n",
      "epoch 190,loss 2.647287368774414\n",
      "tensor(2.6326, grad_fn=<MseLossBackward>)\n",
      "epoch 191,loss 2.632636308670044\n",
      "tensor(2.6181, grad_fn=<MseLossBackward>)\n",
      "epoch 192,loss 2.6181294918060303\n",
      "tensor(2.6038, grad_fn=<MseLossBackward>)\n",
      "epoch 193,loss 2.603766441345215\n",
      "tensor(2.5895, grad_fn=<MseLossBackward>)\n",
      "epoch 194,loss 2.589545965194702\n",
      "tensor(2.5755, grad_fn=<MseLossBackward>)\n",
      "epoch 195,loss 2.5754666328430176\n",
      "tensor(2.5615, grad_fn=<MseLossBackward>)\n",
      "epoch 196,loss 2.561526298522949\n",
      "tensor(2.5477, grad_fn=<MseLossBackward>)\n",
      "epoch 197,loss 2.5477237701416016\n",
      "tensor(2.5341, grad_fn=<MseLossBackward>)\n",
      "epoch 198,loss 2.5340585708618164\n",
      "tensor(2.5205, grad_fn=<MseLossBackward>)\n",
      "epoch 199,loss 2.5205283164978027\n",
      "tensor(2.5071, grad_fn=<MseLossBackward>)\n",
      "epoch 200,loss 2.507132053375244\n",
      "tensor(2.4939, grad_fn=<MseLossBackward>)\n",
      "epoch 201,loss 2.493868589401245\n",
      "tensor(2.4807, grad_fn=<MseLossBackward>)\n",
      "epoch 202,loss 2.480736494064331\n",
      "tensor(2.4677, grad_fn=<MseLossBackward>)\n",
      "epoch 203,loss 2.4677345752716064\n",
      "tensor(2.4549, grad_fn=<MseLossBackward>)\n",
      "epoch 204,loss 2.4548611640930176\n",
      "tensor(2.4421, grad_fn=<MseLossBackward>)\n",
      "epoch 205,loss 2.442115306854248\n",
      "tensor(2.4295, grad_fn=<MseLossBackward>)\n",
      "epoch 206,loss 2.4294960498809814\n",
      "tensor(2.4170, grad_fn=<MseLossBackward>)\n",
      "epoch 207,loss 2.4170010089874268\n",
      "tensor(2.4046, grad_fn=<MseLossBackward>)\n",
      "epoch 208,loss 2.404630422592163\n",
      "tensor(2.3924, grad_fn=<MseLossBackward>)\n",
      "epoch 209,loss 2.3923816680908203\n",
      "tensor(2.3803, grad_fn=<MseLossBackward>)\n",
      "epoch 210,loss 2.3802547454833984\n",
      "tensor(2.3682, grad_fn=<MseLossBackward>)\n",
      "epoch 211,loss 2.3682475090026855\n",
      "tensor(2.3564, grad_fn=<MseLossBackward>)\n",
      "epoch 212,loss 2.3563599586486816\n",
      "tensor(2.3446, grad_fn=<MseLossBackward>)\n",
      "epoch 213,loss 2.3445897102355957\n",
      "tensor(2.3329, grad_fn=<MseLossBackward>)\n",
      "epoch 214,loss 2.3329358100891113\n",
      "tensor(2.3214, grad_fn=<MseLossBackward>)\n",
      "epoch 215,loss 2.321397542953491\n",
      "tensor(2.3100, grad_fn=<MseLossBackward>)\n",
      "epoch 216,loss 2.3099732398986816\n",
      "tensor(2.2987, grad_fn=<MseLossBackward>)\n",
      "epoch 217,loss 2.2986621856689453\n",
      "tensor(2.2875, grad_fn=<MseLossBackward>)\n",
      "epoch 218,loss 2.287464141845703\n",
      "tensor(2.2764, grad_fn=<MseLossBackward>)\n",
      "epoch 219,loss 2.2763757705688477\n",
      "tensor(2.2654, grad_fn=<MseLossBackward>)\n",
      "epoch 220,loss 2.265397787094116\n",
      "tensor(2.2545, grad_fn=<MseLossBackward>)\n",
      "epoch 221,loss 2.254528284072876\n",
      "tensor(2.2438, grad_fn=<MseLossBackward>)\n",
      "epoch 222,loss 2.2437660694122314\n",
      "tensor(2.2331, grad_fn=<MseLossBackward>)\n",
      "epoch 223,loss 2.2331106662750244\n",
      "tensor(2.2226, grad_fn=<MseLossBackward>)\n",
      "epoch 224,loss 2.2225611209869385\n",
      "tensor(2.2121, grad_fn=<MseLossBackward>)\n",
      "epoch 225,loss 2.212115526199341\n",
      "tensor(2.2018, grad_fn=<MseLossBackward>)\n",
      "epoch 226,loss 2.2017738819122314\n",
      "tensor(2.1915, grad_fn=<MseLossBackward>)\n",
      "epoch 227,loss 2.1915342807769775\n",
      "tensor(2.1814, grad_fn=<MseLossBackward>)\n",
      "epoch 228,loss 2.181396245956421\n",
      "tensor(2.1714, grad_fn=<MseLossBackward>)\n",
      "epoch 229,loss 2.171359062194824\n",
      "tensor(2.1614, grad_fn=<MseLossBackward>)\n",
      "epoch 230,loss 2.1614208221435547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1516, grad_fn=<MseLossBackward>)\n",
      "epoch 231,loss 2.151581048965454\n",
      "tensor(2.1418, grad_fn=<MseLossBackward>)\n",
      "epoch 232,loss 2.141838312149048\n",
      "tensor(2.1322, grad_fn=<MseLossBackward>)\n",
      "epoch 233,loss 2.132193088531494\n",
      "tensor(2.1226, grad_fn=<MseLossBackward>)\n",
      "epoch 234,loss 2.1226425170898438\n",
      "tensor(2.1132, grad_fn=<MseLossBackward>)\n",
      "epoch 235,loss 2.1131865978240967\n",
      "tensor(2.1038, grad_fn=<MseLossBackward>)\n",
      "epoch 236,loss 2.1038243770599365\n",
      "tensor(2.0946, grad_fn=<MseLossBackward>)\n",
      "epoch 237,loss 2.094555139541626\n",
      "tensor(2.0854, grad_fn=<MseLossBackward>)\n",
      "epoch 238,loss 2.0853776931762695\n",
      "tensor(2.0763, grad_fn=<MseLossBackward>)\n",
      "epoch 239,loss 2.0762903690338135\n",
      "tensor(2.0673, grad_fn=<MseLossBackward>)\n",
      "epoch 240,loss 2.067293643951416\n",
      "tensor(2.0584, grad_fn=<MseLossBackward>)\n",
      "epoch 241,loss 2.0583858489990234\n",
      "tensor(2.0496, grad_fn=<MseLossBackward>)\n",
      "epoch 242,loss 2.0495667457580566\n",
      "tensor(2.0408, grad_fn=<MseLossBackward>)\n",
      "epoch 243,loss 2.040834903717041\n",
      "tensor(2.0322, grad_fn=<MseLossBackward>)\n",
      "epoch 244,loss 2.03218936920166\n",
      "tensor(2.0236, grad_fn=<MseLossBackward>)\n",
      "epoch 245,loss 2.0236287117004395\n",
      "tensor(2.0152, grad_fn=<MseLossBackward>)\n",
      "epoch 246,loss 2.0151538848876953\n",
      "tensor(2.0068, grad_fn=<MseLossBackward>)\n",
      "epoch 247,loss 2.0067625045776367\n",
      "tensor(1.9985, grad_fn=<MseLossBackward>)\n",
      "epoch 248,loss 1.9984545707702637\n",
      "tensor(1.9902, grad_fn=<MseLossBackward>)\n",
      "epoch 249,loss 1.9902287721633911\n",
      "tensor(1.9821, grad_fn=<MseLossBackward>)\n",
      "epoch 250,loss 1.9820839166641235\n",
      "tensor(1.9740, grad_fn=<MseLossBackward>)\n",
      "epoch 251,loss 1.9740203619003296\n",
      "tensor(1.9660, grad_fn=<MseLossBackward>)\n",
      "epoch 252,loss 1.9660359621047974\n",
      "tensor(1.9581, grad_fn=<MseLossBackward>)\n",
      "epoch 253,loss 1.9581310749053955\n",
      "tensor(1.9503, grad_fn=<MseLossBackward>)\n",
      "epoch 254,loss 1.9503052234649658\n",
      "tensor(1.9426, grad_fn=<MseLossBackward>)\n",
      "epoch 255,loss 1.9425554275512695\n",
      "tensor(1.9349, grad_fn=<MseLossBackward>)\n",
      "epoch 256,loss 1.9348832368850708\n",
      "tensor(1.9273, grad_fn=<MseLossBackward>)\n",
      "epoch 257,loss 1.9272874593734741\n",
      "tensor(1.9198, grad_fn=<MseLossBackward>)\n",
      "epoch 258,loss 1.9197654724121094\n",
      "tensor(1.9123, grad_fn=<MseLossBackward>)\n",
      "epoch 259,loss 1.9123196601867676\n",
      "tensor(1.9049, grad_fn=<MseLossBackward>)\n",
      "epoch 260,loss 1.9049463272094727\n",
      "tensor(1.8976, grad_fn=<MseLossBackward>)\n",
      "epoch 261,loss 1.8976467847824097\n",
      "tensor(1.8904, grad_fn=<MseLossBackward>)\n",
      "epoch 262,loss 1.8904194831848145\n",
      "tensor(1.8833, grad_fn=<MseLossBackward>)\n",
      "epoch 263,loss 1.883263111114502\n",
      "tensor(1.8762, grad_fn=<MseLossBackward>)\n",
      "epoch 264,loss 1.8761777877807617\n",
      "tensor(1.8692, grad_fn=<MseLossBackward>)\n",
      "epoch 265,loss 1.869162917137146\n",
      "tensor(1.8622, grad_fn=<MseLossBackward>)\n",
      "epoch 266,loss 1.8622170686721802\n",
      "tensor(1.8553, grad_fn=<MseLossBackward>)\n",
      "epoch 267,loss 1.855340600013733\n",
      "tensor(1.8485, grad_fn=<MseLossBackward>)\n",
      "epoch 268,loss 1.8485321998596191\n",
      "tensor(1.8418, grad_fn=<MseLossBackward>)\n",
      "epoch 269,loss 1.8417913913726807\n",
      "tensor(1.8351, grad_fn=<MseLossBackward>)\n",
      "epoch 270,loss 1.8351166248321533\n",
      "tensor(1.8285, grad_fn=<MseLossBackward>)\n",
      "epoch 271,loss 1.8285082578659058\n",
      "tensor(1.8220, grad_fn=<MseLossBackward>)\n",
      "epoch 272,loss 1.8219653367996216\n",
      "tensor(1.8155, grad_fn=<MseLossBackward>)\n",
      "epoch 273,loss 1.815487265586853\n",
      "tensor(1.8091, grad_fn=<MseLossBackward>)\n",
      "epoch 274,loss 1.809073805809021\n",
      "tensor(1.8027, grad_fn=<MseLossBackward>)\n",
      "epoch 275,loss 1.8027229309082031\n",
      "tensor(1.7964, grad_fn=<MseLossBackward>)\n",
      "epoch 276,loss 1.7964359521865845\n",
      "tensor(1.7902, grad_fn=<MseLossBackward>)\n",
      "epoch 277,loss 1.7902097702026367\n",
      "tensor(1.7840, grad_fn=<MseLossBackward>)\n",
      "epoch 278,loss 1.7840467691421509\n",
      "tensor(1.7779, grad_fn=<MseLossBackward>)\n",
      "epoch 279,loss 1.7779440879821777\n",
      "tensor(1.7719, grad_fn=<MseLossBackward>)\n",
      "epoch 280,loss 1.7719019651412964\n",
      "tensor(1.7659, grad_fn=<MseLossBackward>)\n",
      "epoch 281,loss 1.7659202814102173\n",
      "tensor(1.7600, grad_fn=<MseLossBackward>)\n",
      "epoch 282,loss 1.7599968910217285\n",
      "tensor(1.7541, grad_fn=<MseLossBackward>)\n",
      "epoch 283,loss 1.7541325092315674\n",
      "tensor(1.7483, grad_fn=<MseLossBackward>)\n",
      "epoch 284,loss 1.7483265399932861\n",
      "tensor(1.7426, grad_fn=<MseLossBackward>)\n",
      "epoch 285,loss 1.7425776720046997\n",
      "tensor(1.7369, grad_fn=<MseLossBackward>)\n",
      "epoch 286,loss 1.73688542842865\n",
      "tensor(1.7313, grad_fn=<MseLossBackward>)\n",
      "epoch 287,loss 1.7312501668930054\n",
      "tensor(1.7257, grad_fn=<MseLossBackward>)\n",
      "epoch 288,loss 1.7256704568862915\n",
      "tensor(1.7201, grad_fn=<MseLossBackward>)\n",
      "epoch 289,loss 1.72014582157135\n",
      "tensor(1.7147, grad_fn=<MseLossBackward>)\n",
      "epoch 290,loss 1.714676022529602\n",
      "tensor(1.7093, grad_fn=<MseLossBackward>)\n",
      "epoch 291,loss 1.7092607021331787\n",
      "tensor(1.7039, grad_fn=<MseLossBackward>)\n",
      "epoch 292,loss 1.7038989067077637\n",
      "tensor(1.6986, grad_fn=<MseLossBackward>)\n",
      "epoch 293,loss 1.6985902786254883\n",
      "tensor(1.6933, grad_fn=<MseLossBackward>)\n",
      "epoch 294,loss 1.6933341026306152\n",
      "tensor(1.6881, grad_fn=<MseLossBackward>)\n",
      "epoch 295,loss 1.6881294250488281\n",
      "tensor(1.6830, grad_fn=<MseLossBackward>)\n",
      "epoch 296,loss 1.6829769611358643\n",
      "tensor(1.6779, grad_fn=<MseLossBackward>)\n",
      "epoch 297,loss 1.6778751611709595\n",
      "tensor(1.6728, grad_fn=<MseLossBackward>)\n",
      "epoch 298,loss 1.6728241443634033\n",
      "tensor(1.6678, grad_fn=<MseLossBackward>)\n",
      "epoch 299,loss 1.6678228378295898\n",
      "tensor(1.6629, grad_fn=<MseLossBackward>)\n",
      "epoch 300,loss 1.6628715991973877\n",
      "tensor(1.6580, grad_fn=<MseLossBackward>)\n",
      "epoch 301,loss 1.657969355583191\n",
      "tensor(1.6531, grad_fn=<MseLossBackward>)\n",
      "epoch 302,loss 1.6531152725219727\n",
      "tensor(1.6483, grad_fn=<MseLossBackward>)\n",
      "epoch 303,loss 1.648309350013733\n",
      "tensor(1.6436, grad_fn=<MseLossBackward>)\n",
      "epoch 304,loss 1.643551230430603\n",
      "tensor(1.6388, grad_fn=<MseLossBackward>)\n",
      "epoch 305,loss 1.6388404369354248\n",
      "tensor(1.6342, grad_fn=<MseLossBackward>)\n",
      "epoch 306,loss 1.6341753005981445\n",
      "tensor(1.6296, grad_fn=<MseLossBackward>)\n",
      "epoch 307,loss 1.6295571327209473\n",
      "tensor(1.6250, grad_fn=<MseLossBackward>)\n",
      "epoch 308,loss 1.6249847412109375\n",
      "tensor(1.6205, grad_fn=<MseLossBackward>)\n",
      "epoch 309,loss 1.6204571723937988\n",
      "tensor(1.6160, grad_fn=<MseLossBackward>)\n",
      "epoch 310,loss 1.6159745454788208\n",
      "tensor(1.6115, grad_fn=<MseLossBackward>)\n",
      "epoch 311,loss 1.611536979675293\n",
      "tensor(1.6071, grad_fn=<MseLossBackward>)\n",
      "epoch 312,loss 1.607142448425293\n",
      "tensor(1.6028, grad_fn=<MseLossBackward>)\n",
      "epoch 313,loss 1.6027919054031372\n",
      "tensor(1.5985, grad_fn=<MseLossBackward>)\n",
      "epoch 314,loss 1.5984843969345093\n",
      "tensor(1.5942, grad_fn=<MseLossBackward>)\n",
      "epoch 315,loss 1.59421968460083\n",
      "tensor(1.5900, grad_fn=<MseLossBackward>)\n",
      "epoch 316,loss 1.5899969339370728\n",
      "tensor(1.5858, grad_fn=<MseLossBackward>)\n",
      "epoch 317,loss 1.5858157873153687\n",
      "tensor(1.5817, grad_fn=<MseLossBackward>)\n",
      "epoch 318,loss 1.581676959991455\n",
      "tensor(1.5776, grad_fn=<MseLossBackward>)\n",
      "epoch 319,loss 1.5775783061981201\n",
      "tensor(1.5735, grad_fn=<MseLossBackward>)\n",
      "epoch 320,loss 1.573520541191101\n",
      "tensor(1.5695, grad_fn=<MseLossBackward>)\n",
      "epoch 321,loss 1.569502592086792\n",
      "tensor(1.5655, grad_fn=<MseLossBackward>)\n",
      "epoch 322,loss 1.5655245780944824\n",
      "tensor(1.5616, grad_fn=<MseLossBackward>)\n",
      "epoch 323,loss 1.5615864992141724\n",
      "tensor(1.5577, grad_fn=<MseLossBackward>)\n",
      "epoch 324,loss 1.5576870441436768\n",
      "tensor(1.5538, grad_fn=<MseLossBackward>)\n",
      "epoch 325,loss 1.553826093673706\n",
      "tensor(1.5500, grad_fn=<MseLossBackward>)\n",
      "epoch 326,loss 1.5500036478042603\n",
      "tensor(1.5462, grad_fn=<MseLossBackward>)\n",
      "epoch 327,loss 1.5462185144424438\n",
      "tensor(1.5425, grad_fn=<MseLossBackward>)\n",
      "epoch 328,loss 1.5424712896347046\n",
      "tensor(1.5388, grad_fn=<MseLossBackward>)\n",
      "epoch 329,loss 1.5387616157531738\n",
      "tensor(1.5351, grad_fn=<MseLossBackward>)\n",
      "epoch 330,loss 1.535088062286377\n",
      "tensor(1.5315, grad_fn=<MseLossBackward>)\n",
      "epoch 331,loss 1.5314509868621826\n",
      "tensor(1.5278, grad_fn=<MseLossBackward>)\n",
      "epoch 332,loss 1.5278496742248535\n",
      "tensor(1.5243, grad_fn=<MseLossBackward>)\n",
      "epoch 333,loss 1.5242843627929688\n",
      "tensor(1.5208, grad_fn=<MseLossBackward>)\n",
      "epoch 334,loss 1.520754337310791\n",
      "tensor(1.5173, grad_fn=<MseLossBackward>)\n",
      "epoch 335,loss 1.5172594785690308\n",
      "tensor(1.5138, grad_fn=<MseLossBackward>)\n",
      "epoch 336,loss 1.513798475265503\n",
      "tensor(1.5104, grad_fn=<MseLossBackward>)\n",
      "epoch 337,loss 1.5103726387023926\n",
      "tensor(1.5070, grad_fn=<MseLossBackward>)\n",
      "epoch 338,loss 1.506980538368225\n",
      "tensor(1.5036, grad_fn=<MseLossBackward>)\n",
      "epoch 339,loss 1.5036221742630005\n",
      "tensor(1.5003, grad_fn=<MseLossBackward>)\n",
      "epoch 340,loss 1.5002965927124023\n",
      "tensor(1.4970, grad_fn=<MseLossBackward>)\n",
      "epoch 341,loss 1.4970037937164307\n",
      "tensor(1.4937, grad_fn=<MseLossBackward>)\n",
      "epoch 342,loss 1.4937444925308228\n",
      "tensor(1.4905, grad_fn=<MseLossBackward>)\n",
      "epoch 343,loss 1.4905165433883667\n",
      "tensor(1.4873, grad_fn=<MseLossBackward>)\n",
      "epoch 344,loss 1.4873210191726685\n",
      "tensor(1.4842, grad_fn=<MseLossBackward>)\n",
      "epoch 345,loss 1.4841569662094116\n",
      "tensor(1.4810, grad_fn=<MseLossBackward>)\n",
      "epoch 346,loss 1.4810240268707275\n",
      "tensor(1.4779, grad_fn=<MseLossBackward>)\n",
      "epoch 347,loss 1.477922797203064\n",
      "tensor(1.4749, grad_fn=<MseLossBackward>)\n",
      "epoch 348,loss 1.4748518466949463\n",
      "tensor(1.4718, grad_fn=<MseLossBackward>)\n",
      "epoch 349,loss 1.4718115329742432\n",
      "tensor(1.4688, grad_fn=<MseLossBackward>)\n",
      "epoch 350,loss 1.4688012599945068\n",
      "tensor(1.4658, grad_fn=<MseLossBackward>)\n",
      "epoch 351,loss 1.4658209085464478\n",
      "tensor(1.4629, grad_fn=<MseLossBackward>)\n",
      "epoch 352,loss 1.4628697633743286\n",
      "tensor(1.4599, grad_fn=<MseLossBackward>)\n",
      "epoch 353,loss 1.4599474668502808\n",
      "tensor(1.4571, grad_fn=<MseLossBackward>)\n",
      "epoch 354,loss 1.4570550918579102\n",
      "tensor(1.4542, grad_fn=<MseLossBackward>)\n",
      "epoch 355,loss 1.4541904926300049\n",
      "tensor(1.4514, grad_fn=<MseLossBackward>)\n",
      "epoch 356,loss 1.4513543844223022\n",
      "tensor(1.4485, grad_fn=<MseLossBackward>)\n",
      "epoch 357,loss 1.4485466480255127\n",
      "tensor(1.4458, grad_fn=<MseLossBackward>)\n",
      "epoch 358,loss 1.4457664489746094\n",
      "tensor(1.4430, grad_fn=<MseLossBackward>)\n",
      "epoch 359,loss 1.4430142641067505\n",
      "tensor(1.4403, grad_fn=<MseLossBackward>)\n",
      "epoch 360,loss 1.4402886629104614\n",
      "tensor(1.4376, grad_fn=<MseLossBackward>)\n",
      "epoch 361,loss 1.4375905990600586\n",
      "tensor(1.4349, grad_fn=<MseLossBackward>)\n",
      "epoch 362,loss 1.4349192380905151\n",
      "tensor(1.4323, grad_fn=<MseLossBackward>)\n",
      "epoch 363,loss 1.432274341583252\n",
      "tensor(1.4297, grad_fn=<MseLossBackward>)\n",
      "epoch 364,loss 1.4296554327011108\n",
      "tensor(1.4271, grad_fn=<MseLossBackward>)\n",
      "epoch 365,loss 1.4270623922348022\n",
      "tensor(1.4245, grad_fn=<MseLossBackward>)\n",
      "epoch 366,loss 1.4244951009750366\n",
      "tensor(1.4220, grad_fn=<MseLossBackward>)\n",
      "epoch 367,loss 1.4219533205032349\n",
      "tensor(1.4194, grad_fn=<MseLossBackward>)\n",
      "epoch 368,loss 1.4194366931915283\n",
      "tensor(1.4169, grad_fn=<MseLossBackward>)\n",
      "epoch 369,loss 1.4169448614120483\n",
      "tensor(1.4145, grad_fn=<MseLossBackward>)\n",
      "epoch 370,loss 1.4144779443740845\n",
      "tensor(1.4120, grad_fn=<MseLossBackward>)\n",
      "epoch 371,loss 1.4120352268218994\n",
      "tensor(1.4096, grad_fn=<MseLossBackward>)\n",
      "epoch 372,loss 1.4096168279647827\n",
      "tensor(1.4072, grad_fn=<MseLossBackward>)\n",
      "epoch 373,loss 1.4072222709655762\n",
      "tensor(1.4049, grad_fn=<MseLossBackward>)\n",
      "epoch 374,loss 1.404852032661438\n",
      "tensor(1.4025, grad_fn=<MseLossBackward>)\n",
      "epoch 375,loss 1.402504563331604\n",
      "tensor(1.4002, grad_fn=<MseLossBackward>)\n",
      "epoch 376,loss 1.4001803398132324\n",
      "tensor(1.3979, grad_fn=<MseLossBackward>)\n",
      "epoch 377,loss 1.3978794813156128\n",
      "tensor(1.3956, grad_fn=<MseLossBackward>)\n",
      "epoch 378,loss 1.3956013917922974\n",
      "tensor(1.3933, grad_fn=<MseLossBackward>)\n",
      "epoch 379,loss 1.3933453559875488\n",
      "tensor(1.3911, grad_fn=<MseLossBackward>)\n",
      "epoch 380,loss 1.3911125659942627\n",
      "tensor(1.3889, grad_fn=<MseLossBackward>)\n",
      "epoch 381,loss 1.388900876045227\n",
      "tensor(1.3867, grad_fn=<MseLossBackward>)\n",
      "epoch 382,loss 1.386711597442627\n",
      "tensor(1.3845, grad_fn=<MseLossBackward>)\n",
      "epoch 383,loss 1.3845438957214355\n",
      "tensor(1.3824, grad_fn=<MseLossBackward>)\n",
      "epoch 384,loss 1.382398247718811\n",
      "tensor(1.3803, grad_fn=<MseLossBackward>)\n",
      "epoch 385,loss 1.3802729845046997\n",
      "tensor(1.3782, grad_fn=<MseLossBackward>)\n",
      "epoch 386,loss 1.378169298171997\n",
      "tensor(1.3761, grad_fn=<MseLossBackward>)\n",
      "epoch 387,loss 1.3760864734649658\n",
      "tensor(1.3740, grad_fn=<MseLossBackward>)\n",
      "epoch 388,loss 1.3740234375\n",
      "tensor(1.3720, grad_fn=<MseLossBackward>)\n",
      "epoch 389,loss 1.3719816207885742\n",
      "tensor(1.3700, grad_fn=<MseLossBackward>)\n",
      "epoch 390,loss 1.3699601888656616\n",
      "tensor(1.3680, grad_fn=<MseLossBackward>)\n",
      "epoch 391,loss 1.367958426475525\n",
      "tensor(1.3660, grad_fn=<MseLossBackward>)\n",
      "epoch 392,loss 1.3659765720367432\n",
      "tensor(1.3640, grad_fn=<MseLossBackward>)\n",
      "epoch 393,loss 1.3640140295028687\n",
      "tensor(1.3621, grad_fn=<MseLossBackward>)\n",
      "epoch 394,loss 1.3620712757110596\n",
      "tensor(1.3601, grad_fn=<MseLossBackward>)\n",
      "epoch 395,loss 1.3601479530334473\n",
      "tensor(1.3582, grad_fn=<MseLossBackward>)\n",
      "epoch 396,loss 1.3582432270050049\n",
      "tensor(1.3564, grad_fn=<MseLossBackward>)\n",
      "epoch 397,loss 1.356357455253601\n",
      "tensor(1.3545, grad_fn=<MseLossBackward>)\n",
      "epoch 398,loss 1.3544905185699463\n",
      "tensor(1.3526, grad_fn=<MseLossBackward>)\n",
      "epoch 399,loss 1.3526420593261719\n",
      "tensor(1.3508, grad_fn=<MseLossBackward>)\n",
      "epoch 400,loss 1.3508120775222778\n",
      "tensor(1.3490, grad_fn=<MseLossBackward>)\n",
      "epoch 401,loss 1.3489996194839478\n",
      "tensor(1.3472, grad_fn=<MseLossBackward>)\n",
      "epoch 402,loss 1.3472057580947876\n",
      "tensor(1.3454, grad_fn=<MseLossBackward>)\n",
      "epoch 403,loss 1.3454291820526123\n",
      "tensor(1.3437, grad_fn=<MseLossBackward>)\n",
      "epoch 404,loss 1.3436704874038696\n",
      "tensor(1.3419, grad_fn=<MseLossBackward>)\n",
      "epoch 405,loss 1.3419289588928223\n",
      "tensor(1.3402, grad_fn=<MseLossBackward>)\n",
      "epoch 406,loss 1.340205192565918\n",
      "tensor(1.3385, grad_fn=<MseLossBackward>)\n",
      "epoch 407,loss 1.3384982347488403\n",
      "tensor(1.3368, grad_fn=<MseLossBackward>)\n",
      "epoch 408,loss 1.3368079662322998\n",
      "tensor(1.3351, grad_fn=<MseLossBackward>)\n",
      "epoch 409,loss 1.3351342678070068\n",
      "tensor(1.3335, grad_fn=<MseLossBackward>)\n",
      "epoch 410,loss 1.3334777355194092\n",
      "tensor(1.3318, grad_fn=<MseLossBackward>)\n",
      "epoch 411,loss 1.3318371772766113\n",
      "tensor(1.3302, grad_fn=<MseLossBackward>)\n",
      "epoch 412,loss 1.330213189125061\n",
      "tensor(1.3286, grad_fn=<MseLossBackward>)\n",
      "epoch 413,loss 1.328605055809021\n",
      "tensor(1.3270, grad_fn=<MseLossBackward>)\n",
      "epoch 414,loss 1.3270131349563599\n",
      "tensor(1.3254, grad_fn=<MseLossBackward>)\n",
      "epoch 415,loss 1.3254365921020508\n",
      "tensor(1.3239, grad_fn=<MseLossBackward>)\n",
      "epoch 416,loss 1.3238757848739624\n",
      "tensor(1.3223, grad_fn=<MseLossBackward>)\n",
      "epoch 417,loss 1.3223304748535156\n",
      "tensor(1.3208, grad_fn=<MseLossBackward>)\n",
      "epoch 418,loss 1.3207998275756836\n",
      "tensor(1.3193, grad_fn=<MseLossBackward>)\n",
      "epoch 419,loss 1.3192856311798096\n",
      "tensor(1.3178, grad_fn=<MseLossBackward>)\n",
      "epoch 420,loss 1.317785620689392\n",
      "tensor(1.3163, grad_fn=<MseLossBackward>)\n",
      "epoch 421,loss 1.3163001537322998\n",
      "tensor(1.3148, grad_fn=<MseLossBackward>)\n",
      "epoch 422,loss 1.3148300647735596\n",
      "tensor(1.3134, grad_fn=<MseLossBackward>)\n",
      "epoch 423,loss 1.3133742809295654\n",
      "tensor(1.3119, grad_fn=<MseLossBackward>)\n",
      "epoch 424,loss 1.311933159828186\n",
      "tensor(1.3105, grad_fn=<MseLossBackward>)\n",
      "epoch 425,loss 1.3105063438415527\n",
      "tensor(1.3091, grad_fn=<MseLossBackward>)\n",
      "epoch 426,loss 1.3090931177139282\n",
      "tensor(1.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 427,loss 1.307694673538208\n",
      "tensor(1.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 428,loss 1.3063088655471802\n",
      "tensor(1.3049, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 429,loss 1.3049380779266357\n",
      "tensor(1.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 430,loss 1.303580641746521\n",
      "tensor(1.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 431,loss 1.302235722541809\n",
      "tensor(1.3009, grad_fn=<MseLossBackward>)\n",
      "epoch 432,loss 1.3009045124053955\n",
      "tensor(1.2996, grad_fn=<MseLossBackward>)\n",
      "epoch 433,loss 1.2995868921279907\n",
      "tensor(1.2983, grad_fn=<MseLossBackward>)\n",
      "epoch 434,loss 1.2982821464538574\n",
      "tensor(1.2970, grad_fn=<MseLossBackward>)\n",
      "epoch 435,loss 1.2969903945922852\n",
      "tensor(1.2957, grad_fn=<MseLossBackward>)\n",
      "epoch 436,loss 1.2957115173339844\n",
      "tensor(1.2944, grad_fn=<MseLossBackward>)\n",
      "epoch 437,loss 1.2944450378417969\n",
      "tensor(1.2932, grad_fn=<MseLossBackward>)\n",
      "epoch 438,loss 1.2931910753250122\n",
      "tensor(1.2919, grad_fn=<MseLossBackward>)\n",
      "epoch 439,loss 1.2919498682022095\n",
      "tensor(1.2907, grad_fn=<MseLossBackward>)\n",
      "epoch 440,loss 1.2907204627990723\n",
      "tensor(1.2895, grad_fn=<MseLossBackward>)\n",
      "epoch 441,loss 1.289503574371338\n",
      "tensor(1.2883, grad_fn=<MseLossBackward>)\n",
      "epoch 442,loss 1.288298487663269\n",
      "tensor(1.2871, grad_fn=<MseLossBackward>)\n",
      "epoch 443,loss 1.2871055603027344\n",
      "tensor(1.2859, grad_fn=<MseLossBackward>)\n",
      "epoch 444,loss 1.2859244346618652\n",
      "tensor(1.2848, grad_fn=<MseLossBackward>)\n",
      "epoch 445,loss 1.284754753112793\n",
      "tensor(1.2836, grad_fn=<MseLossBackward>)\n",
      "epoch 446,loss 1.2835972309112549\n",
      "tensor(1.2825, grad_fn=<MseLossBackward>)\n",
      "epoch 447,loss 1.2824504375457764\n",
      "tensor(1.2813, grad_fn=<MseLossBackward>)\n",
      "epoch 448,loss 1.2813152074813843\n",
      "tensor(1.2802, grad_fn=<MseLossBackward>)\n",
      "epoch 449,loss 1.2801913022994995\n",
      "tensor(1.2791, grad_fn=<MseLossBackward>)\n",
      "epoch 450,loss 1.279078483581543\n",
      "tensor(1.2780, grad_fn=<MseLossBackward>)\n",
      "epoch 451,loss 1.2779769897460938\n",
      "tensor(1.2769, grad_fn=<MseLossBackward>)\n",
      "epoch 452,loss 1.2768858671188354\n",
      "tensor(1.2758, grad_fn=<MseLossBackward>)\n",
      "epoch 453,loss 1.2758057117462158\n",
      "tensor(1.2747, grad_fn=<MseLossBackward>)\n",
      "epoch 454,loss 1.274736762046814\n",
      "tensor(1.2737, grad_fn=<MseLossBackward>)\n",
      "epoch 455,loss 1.273678183555603\n",
      "tensor(1.2726, grad_fn=<MseLossBackward>)\n",
      "epoch 456,loss 1.272629976272583\n",
      "tensor(1.2716, grad_fn=<MseLossBackward>)\n",
      "epoch 457,loss 1.2715922594070435\n",
      "tensor(1.2706, grad_fn=<MseLossBackward>)\n",
      "epoch 458,loss 1.2705645561218262\n",
      "tensor(1.2695, grad_fn=<MseLossBackward>)\n",
      "epoch 459,loss 1.2695472240447998\n",
      "tensor(1.2685, grad_fn=<MseLossBackward>)\n",
      "epoch 460,loss 1.2685397863388062\n",
      "tensor(1.2675, grad_fn=<MseLossBackward>)\n",
      "epoch 461,loss 1.267542839050293\n",
      "tensor(1.2666, grad_fn=<MseLossBackward>)\n",
      "epoch 462,loss 1.2665553092956543\n",
      "tensor(1.2656, grad_fn=<MseLossBackward>)\n",
      "epoch 463,loss 1.2655776739120483\n",
      "tensor(1.2646, grad_fn=<MseLossBackward>)\n",
      "epoch 464,loss 1.2646095752716064\n",
      "tensor(1.2637, grad_fn=<MseLossBackward>)\n",
      "epoch 465,loss 1.2636513710021973\n",
      "tensor(1.2627, grad_fn=<MseLossBackward>)\n",
      "epoch 466,loss 1.2627023458480835\n",
      "tensor(1.2618, grad_fn=<MseLossBackward>)\n",
      "epoch 467,loss 1.2617628574371338\n",
      "tensor(1.2608, grad_fn=<MseLossBackward>)\n",
      "epoch 468,loss 1.2608325481414795\n",
      "tensor(1.2599, grad_fn=<MseLossBackward>)\n",
      "epoch 469,loss 1.2599116563796997\n",
      "tensor(1.2590, grad_fn=<MseLossBackward>)\n",
      "epoch 470,loss 1.2589997053146362\n",
      "tensor(1.2581, grad_fn=<MseLossBackward>)\n",
      "epoch 471,loss 1.258096694946289\n",
      "tensor(1.2572, grad_fn=<MseLossBackward>)\n",
      "epoch 472,loss 1.2572029829025269\n",
      "tensor(1.2563, grad_fn=<MseLossBackward>)\n",
      "epoch 473,loss 1.2563180923461914\n",
      "tensor(1.2554, grad_fn=<MseLossBackward>)\n",
      "epoch 474,loss 1.255441665649414\n",
      "tensor(1.2546, grad_fn=<MseLossBackward>)\n",
      "epoch 475,loss 1.254574179649353\n",
      "tensor(1.2537, grad_fn=<MseLossBackward>)\n",
      "epoch 476,loss 1.25371515750885\n",
      "tensor(1.2529, grad_fn=<MseLossBackward>)\n",
      "epoch 477,loss 1.2528645992279053\n",
      "tensor(1.2520, grad_fn=<MseLossBackward>)\n",
      "epoch 478,loss 1.252022385597229\n",
      "tensor(1.2512, grad_fn=<MseLossBackward>)\n",
      "epoch 479,loss 1.2511886358261108\n",
      "tensor(1.2504, grad_fn=<MseLossBackward>)\n",
      "epoch 480,loss 1.2503632307052612\n",
      "tensor(1.2495, grad_fn=<MseLossBackward>)\n",
      "epoch 481,loss 1.249545693397522\n",
      "tensor(1.2487, grad_fn=<MseLossBackward>)\n",
      "epoch 482,loss 1.2487366199493408\n",
      "tensor(1.2479, grad_fn=<MseLossBackward>)\n",
      "epoch 483,loss 1.2479355335235596\n",
      "tensor(1.2471, grad_fn=<MseLossBackward>)\n",
      "epoch 484,loss 1.2471423149108887\n",
      "tensor(1.2464, grad_fn=<MseLossBackward>)\n",
      "epoch 485,loss 1.2463568449020386\n",
      "tensor(1.2456, grad_fn=<MseLossBackward>)\n",
      "epoch 486,loss 1.2455791234970093\n",
      "tensor(1.2448, grad_fn=<MseLossBackward>)\n",
      "epoch 487,loss 1.2448093891143799\n",
      "tensor(1.2440, grad_fn=<MseLossBackward>)\n",
      "epoch 488,loss 1.244046926498413\n",
      "tensor(1.2433, grad_fn=<MseLossBackward>)\n",
      "epoch 489,loss 1.243292212486267\n",
      "tensor(1.2425, grad_fn=<MseLossBackward>)\n",
      "epoch 490,loss 1.2425450086593628\n",
      "tensor(1.2418, grad_fn=<MseLossBackward>)\n",
      "epoch 491,loss 1.2418049573898315\n",
      "tensor(1.2411, grad_fn=<MseLossBackward>)\n",
      "epoch 492,loss 1.2410728931427002\n",
      "tensor(1.2403, grad_fn=<MseLossBackward>)\n",
      "epoch 493,loss 1.2403472661972046\n",
      "tensor(1.2396, grad_fn=<MseLossBackward>)\n",
      "epoch 494,loss 1.2396292686462402\n",
      "tensor(1.2389, grad_fn=<MseLossBackward>)\n",
      "epoch 495,loss 1.2389179468154907\n",
      "tensor(1.2382, grad_fn=<MseLossBackward>)\n",
      "epoch 496,loss 1.2382142543792725\n",
      "tensor(1.2375, grad_fn=<MseLossBackward>)\n",
      "epoch 497,loss 1.2375173568725586\n",
      "tensor(1.2368, grad_fn=<MseLossBackward>)\n",
      "epoch 498,loss 1.23682701587677\n",
      "tensor(1.2361, grad_fn=<MseLossBackward>)\n",
      "epoch 499,loss 1.236143708229065\n",
      "tensor(1.2355, grad_fn=<MseLossBackward>)\n",
      "epoch 500,loss 1.2354674339294434\n",
      "tensor(1.2348, grad_fn=<MseLossBackward>)\n",
      "epoch 501,loss 1.2347973585128784\n",
      "tensor(1.2341, grad_fn=<MseLossBackward>)\n",
      "epoch 502,loss 1.234134316444397\n",
      "tensor(1.2335, grad_fn=<MseLossBackward>)\n",
      "epoch 503,loss 1.2334775924682617\n",
      "tensor(1.2328, grad_fn=<MseLossBackward>)\n",
      "epoch 504,loss 1.2328273057937622\n",
      "tensor(1.2322, grad_fn=<MseLossBackward>)\n",
      "epoch 505,loss 1.2321840524673462\n",
      "tensor(1.2315, grad_fn=<MseLossBackward>)\n",
      "epoch 506,loss 1.2315465211868286\n",
      "tensor(1.2309, grad_fn=<MseLossBackward>)\n",
      "epoch 507,loss 1.2309155464172363\n",
      "tensor(1.2303, grad_fn=<MseLossBackward>)\n",
      "epoch 508,loss 1.2302908897399902\n",
      "tensor(1.2297, grad_fn=<MseLossBackward>)\n",
      "epoch 509,loss 1.2296719551086426\n",
      "tensor(1.2291, grad_fn=<MseLossBackward>)\n",
      "epoch 510,loss 1.2290599346160889\n",
      "tensor(1.2285, grad_fn=<MseLossBackward>)\n",
      "epoch 511,loss 1.2284536361694336\n",
      "tensor(1.2279, grad_fn=<MseLossBackward>)\n",
      "epoch 512,loss 1.2278532981872559\n",
      "tensor(1.2273, grad_fn=<MseLossBackward>)\n",
      "epoch 513,loss 1.2272588014602661\n",
      "tensor(1.2267, grad_fn=<MseLossBackward>)\n",
      "epoch 514,loss 1.226670503616333\n",
      "tensor(1.2261, grad_fn=<MseLossBackward>)\n",
      "epoch 515,loss 1.2260876893997192\n",
      "tensor(1.2255, grad_fn=<MseLossBackward>)\n",
      "epoch 516,loss 1.225511074066162\n",
      "tensor(1.2249, grad_fn=<MseLossBackward>)\n",
      "epoch 517,loss 1.2249397039413452\n",
      "tensor(1.2244, grad_fn=<MseLossBackward>)\n",
      "epoch 518,loss 1.2243740558624268\n",
      "tensor(1.2238, grad_fn=<MseLossBackward>)\n",
      "epoch 519,loss 1.2238143682479858\n",
      "tensor(1.2233, grad_fn=<MseLossBackward>)\n",
      "epoch 520,loss 1.2232599258422852\n",
      "tensor(1.2227, grad_fn=<MseLossBackward>)\n",
      "epoch 521,loss 1.2227109670639038\n",
      "tensor(1.2222, grad_fn=<MseLossBackward>)\n",
      "epoch 522,loss 1.2221674919128418\n",
      "tensor(1.2216, grad_fn=<MseLossBackward>)\n",
      "epoch 523,loss 1.2216295003890991\n",
      "tensor(1.2211, grad_fn=<MseLossBackward>)\n",
      "epoch 524,loss 1.2210965156555176\n",
      "tensor(1.2206, grad_fn=<MseLossBackward>)\n",
      "epoch 525,loss 1.2205692529678345\n",
      "tensor(1.2200, grad_fn=<MseLossBackward>)\n",
      "epoch 526,loss 1.220046877861023\n",
      "tensor(1.2195, grad_fn=<MseLossBackward>)\n",
      "epoch 527,loss 1.2195297479629517\n",
      "tensor(1.2190, grad_fn=<MseLossBackward>)\n",
      "epoch 528,loss 1.219017744064331\n",
      "tensor(1.2185, grad_fn=<MseLossBackward>)\n",
      "epoch 529,loss 1.2185111045837402\n",
      "tensor(1.2180, grad_fn=<MseLossBackward>)\n",
      "epoch 530,loss 1.2180089950561523\n",
      "tensor(1.2175, grad_fn=<MseLossBackward>)\n",
      "epoch 531,loss 1.2175121307373047\n",
      "tensor(1.2170, grad_fn=<MseLossBackward>)\n",
      "epoch 532,loss 1.2170199155807495\n",
      "tensor(1.2165, grad_fn=<MseLossBackward>)\n",
      "epoch 533,loss 1.2165329456329346\n",
      "tensor(1.2161, grad_fn=<MseLossBackward>)\n",
      "epoch 534,loss 1.216050624847412\n",
      "tensor(1.2156, grad_fn=<MseLossBackward>)\n",
      "epoch 535,loss 1.2155731916427612\n",
      "tensor(1.2151, grad_fn=<MseLossBackward>)\n",
      "epoch 536,loss 1.2151005268096924\n",
      "tensor(1.2146, grad_fn=<MseLossBackward>)\n",
      "epoch 537,loss 1.214632272720337\n",
      "tensor(1.2142, grad_fn=<MseLossBackward>)\n",
      "epoch 538,loss 1.2141690254211426\n",
      "tensor(1.2137, grad_fn=<MseLossBackward>)\n",
      "epoch 539,loss 1.213710069656372\n",
      "tensor(1.2133, grad_fn=<MseLossBackward>)\n",
      "epoch 540,loss 1.2132556438446045\n",
      "tensor(1.2128, grad_fn=<MseLossBackward>)\n",
      "epoch 541,loss 1.2128057479858398\n",
      "tensor(1.2124, grad_fn=<MseLossBackward>)\n",
      "epoch 542,loss 1.2123606204986572\n",
      "tensor(1.2119, grad_fn=<MseLossBackward>)\n",
      "epoch 543,loss 1.2119196653366089\n",
      "tensor(1.2115, grad_fn=<MseLossBackward>)\n",
      "epoch 544,loss 1.211483120918274\n",
      "tensor(1.2111, grad_fn=<MseLossBackward>)\n",
      "epoch 545,loss 1.2110505104064941\n",
      "tensor(1.2106, grad_fn=<MseLossBackward>)\n",
      "epoch 546,loss 1.2106224298477173\n",
      "tensor(1.2102, grad_fn=<MseLossBackward>)\n",
      "epoch 547,loss 1.210198998451233\n",
      "tensor(1.2098, grad_fn=<MseLossBackward>)\n",
      "epoch 548,loss 1.2097793817520142\n",
      "tensor(1.2094, grad_fn=<MseLossBackward>)\n",
      "epoch 549,loss 1.209363579750061\n",
      "tensor(1.2090, grad_fn=<MseLossBackward>)\n",
      "epoch 550,loss 1.2089526653289795\n",
      "tensor(1.2085, grad_fn=<MseLossBackward>)\n",
      "epoch 551,loss 1.2085458040237427\n",
      "tensor(1.2081, grad_fn=<MseLossBackward>)\n",
      "epoch 552,loss 1.2081422805786133\n",
      "tensor(1.2077, grad_fn=<MseLossBackward>)\n",
      "epoch 553,loss 1.2077431678771973\n",
      "tensor(1.2073, grad_fn=<MseLossBackward>)\n",
      "epoch 554,loss 1.2073478698730469\n",
      "tensor(1.2070, grad_fn=<MseLossBackward>)\n",
      "epoch 555,loss 1.2069567441940308\n",
      "tensor(1.2066, grad_fn=<MseLossBackward>)\n",
      "epoch 556,loss 1.206568956375122\n",
      "tensor(1.2062, grad_fn=<MseLossBackward>)\n",
      "epoch 557,loss 1.2061858177185059\n",
      "tensor(1.2058, grad_fn=<MseLossBackward>)\n",
      "epoch 558,loss 1.2058056592941284\n",
      "tensor(1.2054, grad_fn=<MseLossBackward>)\n",
      "epoch 559,loss 1.2054297924041748\n",
      "tensor(1.2051, grad_fn=<MseLossBackward>)\n",
      "epoch 560,loss 1.2050573825836182\n",
      "tensor(1.2047, grad_fn=<MseLossBackward>)\n",
      "epoch 561,loss 1.204688549041748\n",
      "tensor(1.2043, grad_fn=<MseLossBackward>)\n",
      "epoch 562,loss 1.2043235301971436\n",
      "tensor(1.2040, grad_fn=<MseLossBackward>)\n",
      "epoch 563,loss 1.203961968421936\n",
      "tensor(1.2036, grad_fn=<MseLossBackward>)\n",
      "epoch 564,loss 1.2036043405532837\n",
      "tensor(1.2033, grad_fn=<MseLossBackward>)\n",
      "epoch 565,loss 1.2032502889633179\n",
      "tensor(1.2029, grad_fn=<MseLossBackward>)\n",
      "epoch 566,loss 1.2028992176055908\n",
      "tensor(1.2026, grad_fn=<MseLossBackward>)\n",
      "epoch 567,loss 1.2025521993637085\n",
      "tensor(1.2022, grad_fn=<MseLossBackward>)\n",
      "epoch 568,loss 1.202208399772644\n",
      "tensor(1.2019, grad_fn=<MseLossBackward>)\n",
      "epoch 569,loss 1.2018680572509766\n",
      "tensor(1.2015, grad_fn=<MseLossBackward>)\n",
      "epoch 570,loss 1.201530933380127\n",
      "tensor(1.2012, grad_fn=<MseLossBackward>)\n",
      "epoch 571,loss 1.2011971473693848\n",
      "tensor(1.2009, grad_fn=<MseLossBackward>)\n",
      "epoch 572,loss 1.2008668184280396\n",
      "tensor(1.2005, grad_fn=<MseLossBackward>)\n",
      "epoch 573,loss 1.2005398273468018\n",
      "tensor(1.2002, grad_fn=<MseLossBackward>)\n",
      "epoch 574,loss 1.2002158164978027\n",
      "tensor(1.1999, grad_fn=<MseLossBackward>)\n",
      "epoch 575,loss 1.1998951435089111\n",
      "tensor(1.1996, grad_fn=<MseLossBackward>)\n",
      "epoch 576,loss 1.1995775699615479\n",
      "tensor(1.1993, grad_fn=<MseLossBackward>)\n",
      "epoch 577,loss 1.199263095855713\n",
      "tensor(1.1990, grad_fn=<MseLossBackward>)\n",
      "epoch 578,loss 1.1989519596099854\n",
      "tensor(1.1986, grad_fn=<MseLossBackward>)\n",
      "epoch 579,loss 1.1986439228057861\n",
      "tensor(1.1983, grad_fn=<MseLossBackward>)\n",
      "epoch 580,loss 1.1983386278152466\n",
      "tensor(1.1980, grad_fn=<MseLossBackward>)\n",
      "epoch 581,loss 1.198036551475525\n",
      "tensor(1.1977, grad_fn=<MseLossBackward>)\n",
      "epoch 582,loss 1.197737455368042\n",
      "tensor(1.1974, grad_fn=<MseLossBackward>)\n",
      "epoch 583,loss 1.1974414587020874\n",
      "tensor(1.1971, grad_fn=<MseLossBackward>)\n",
      "epoch 584,loss 1.197148323059082\n",
      "tensor(1.1969, grad_fn=<MseLossBackward>)\n",
      "epoch 585,loss 1.1968578100204468\n",
      "tensor(1.1966, grad_fn=<MseLossBackward>)\n",
      "epoch 586,loss 1.1965705156326294\n",
      "tensor(1.1963, grad_fn=<MseLossBackward>)\n",
      "epoch 587,loss 1.1962858438491821\n",
      "tensor(1.1960, grad_fn=<MseLossBackward>)\n",
      "epoch 588,loss 1.1960043907165527\n",
      "tensor(1.1957, grad_fn=<MseLossBackward>)\n",
      "epoch 589,loss 1.195725440979004\n",
      "tensor(1.1954, grad_fn=<MseLossBackward>)\n",
      "epoch 590,loss 1.1954487562179565\n",
      "tensor(1.1952, grad_fn=<MseLossBackward>)\n",
      "epoch 591,loss 1.1951755285263062\n",
      "tensor(1.1949, grad_fn=<MseLossBackward>)\n",
      "epoch 592,loss 1.1949046850204468\n",
      "tensor(1.1946, grad_fn=<MseLossBackward>)\n",
      "epoch 593,loss 1.1946367025375366\n",
      "tensor(1.1944, grad_fn=<MseLossBackward>)\n",
      "epoch 594,loss 1.194370985031128\n",
      "tensor(1.1941, grad_fn=<MseLossBackward>)\n",
      "epoch 595,loss 1.194108247756958\n",
      "tensor(1.1938, grad_fn=<MseLossBackward>)\n",
      "epoch 596,loss 1.1938480138778687\n",
      "tensor(1.1936, grad_fn=<MseLossBackward>)\n",
      "epoch 597,loss 1.1935908794403076\n",
      "tensor(1.1933, grad_fn=<MseLossBackward>)\n",
      "epoch 598,loss 1.1933354139328003\n",
      "tensor(1.1931, grad_fn=<MseLossBackward>)\n",
      "epoch 599,loss 1.1930828094482422\n",
      "tensor(1.1928, grad_fn=<MseLossBackward>)\n",
      "epoch 600,loss 1.1928328275680542\n",
      "tensor(1.1926, grad_fn=<MseLossBackward>)\n",
      "epoch 601,loss 1.1925854682922363\n",
      "tensor(1.1923, grad_fn=<MseLossBackward>)\n",
      "epoch 602,loss 1.1923402547836304\n",
      "tensor(1.1921, grad_fn=<MseLossBackward>)\n",
      "epoch 603,loss 1.192097544670105\n",
      "tensor(1.1919, grad_fn=<MseLossBackward>)\n",
      "epoch 604,loss 1.1918573379516602\n",
      "tensor(1.1916, grad_fn=<MseLossBackward>)\n",
      "epoch 605,loss 1.1916193962097168\n",
      "tensor(1.1914, grad_fn=<MseLossBackward>)\n",
      "epoch 606,loss 1.191383719444275\n",
      "tensor(1.1912, grad_fn=<MseLossBackward>)\n",
      "epoch 607,loss 1.1911506652832031\n",
      "tensor(1.1909, grad_fn=<MseLossBackward>)\n",
      "epoch 608,loss 1.1909195184707642\n",
      "tensor(1.1907, grad_fn=<MseLossBackward>)\n",
      "epoch 609,loss 1.1906912326812744\n",
      "tensor(1.1905, grad_fn=<MseLossBackward>)\n",
      "epoch 610,loss 1.1904643774032593\n",
      "tensor(1.1902, grad_fn=<MseLossBackward>)\n",
      "epoch 611,loss 1.190240740776062\n",
      "tensor(1.1900, grad_fn=<MseLossBackward>)\n",
      "epoch 612,loss 1.1900187730789185\n",
      "tensor(1.1898, grad_fn=<MseLossBackward>)\n",
      "epoch 613,loss 1.1897989511489868\n",
      "tensor(1.1896, grad_fn=<MseLossBackward>)\n",
      "epoch 614,loss 1.1895816326141357\n",
      "tensor(1.1894, grad_fn=<MseLossBackward>)\n",
      "epoch 615,loss 1.1893658638000488\n",
      "tensor(1.1892, grad_fn=<MseLossBackward>)\n",
      "epoch 616,loss 1.189152717590332\n",
      "tensor(1.1889, grad_fn=<MseLossBackward>)\n",
      "epoch 617,loss 1.1889418363571167\n",
      "tensor(1.1887, grad_fn=<MseLossBackward>)\n",
      "epoch 618,loss 1.1887328624725342\n",
      "tensor(1.1885, grad_fn=<MseLossBackward>)\n",
      "epoch 619,loss 1.188525915145874\n",
      "tensor(1.1883, grad_fn=<MseLossBackward>)\n",
      "epoch 620,loss 1.1883209943771362\n",
      "tensor(1.1881, grad_fn=<MseLossBackward>)\n",
      "epoch 621,loss 1.1881178617477417\n",
      "tensor(1.1879, grad_fn=<MseLossBackward>)\n",
      "epoch 622,loss 1.1879171133041382\n",
      "tensor(1.1877, grad_fn=<MseLossBackward>)\n",
      "epoch 623,loss 1.187718152999878\n",
      "tensor(1.1875, grad_fn=<MseLossBackward>)\n",
      "epoch 624,loss 1.1875213384628296\n",
      "tensor(1.1873, grad_fn=<MseLossBackward>)\n",
      "epoch 625,loss 1.187326431274414\n",
      "tensor(1.1871, grad_fn=<MseLossBackward>)\n",
      "epoch 626,loss 1.1871334314346313\n",
      "tensor(1.1869, grad_fn=<MseLossBackward>)\n",
      "epoch 627,loss 1.1869423389434814\n",
      "tensor(1.1868, grad_fn=<MseLossBackward>)\n",
      "epoch 628,loss 1.1867529153823853\n",
      "tensor(1.1866, grad_fn=<MseLossBackward>)\n",
      "epoch 629,loss 1.186565637588501\n",
      "tensor(1.1864, grad_fn=<MseLossBackward>)\n",
      "epoch 630,loss 1.1863800287246704\n",
      "tensor(1.1862, grad_fn=<MseLossBackward>)\n",
      "epoch 631,loss 1.1861965656280518\n",
      "tensor(1.1860, grad_fn=<MseLossBackward>)\n",
      "epoch 632,loss 1.1860145330429077\n",
      "tensor(1.1858, grad_fn=<MseLossBackward>)\n",
      "epoch 633,loss 1.185834527015686\n",
      "tensor(1.1857, grad_fn=<MseLossBackward>)\n",
      "epoch 634,loss 1.1856564283370972\n",
      "tensor(1.1855, grad_fn=<MseLossBackward>)\n",
      "epoch 635,loss 1.185479998588562\n",
      "tensor(1.1853, grad_fn=<MseLossBackward>)\n",
      "epoch 636,loss 1.185305118560791\n",
      "tensor(1.1851, grad_fn=<MseLossBackward>)\n",
      "epoch 637,loss 1.1851322650909424\n",
      "tensor(1.1850, grad_fn=<MseLossBackward>)\n",
      "epoch 638,loss 1.184960961341858\n",
      "tensor(1.1848, grad_fn=<MseLossBackward>)\n",
      "epoch 639,loss 1.1847909688949585\n",
      "tensor(1.1846, grad_fn=<MseLossBackward>)\n",
      "epoch 640,loss 1.1846232414245605\n",
      "tensor(1.1845, grad_fn=<MseLossBackward>)\n",
      "epoch 641,loss 1.1844570636749268\n",
      "tensor(1.1843, grad_fn=<MseLossBackward>)\n",
      "epoch 642,loss 1.1842924356460571\n",
      "tensor(1.1841, grad_fn=<MseLossBackward>)\n",
      "epoch 643,loss 1.1841293573379517\n",
      "tensor(1.1840, grad_fn=<MseLossBackward>)\n",
      "epoch 644,loss 1.1839679479599\n",
      "tensor(1.1838, grad_fn=<MseLossBackward>)\n",
      "epoch 645,loss 1.1838080883026123\n",
      "tensor(1.1837, grad_fn=<MseLossBackward>)\n",
      "epoch 646,loss 1.183650016784668\n",
      "tensor(1.1835, grad_fn=<MseLossBackward>)\n",
      "epoch 647,loss 1.1834933757781982\n",
      "tensor(1.1833, grad_fn=<MseLossBackward>)\n",
      "epoch 648,loss 1.1833384037017822\n",
      "tensor(1.1832, grad_fn=<MseLossBackward>)\n",
      "epoch 649,loss 1.1831845045089722\n",
      "tensor(1.1830, grad_fn=<MseLossBackward>)\n",
      "epoch 650,loss 1.1830328702926636\n",
      "tensor(1.1829, grad_fn=<MseLossBackward>)\n",
      "epoch 651,loss 1.1828824281692505\n",
      "tensor(1.1827, grad_fn=<MseLossBackward>)\n",
      "epoch 652,loss 1.1827330589294434\n",
      "tensor(1.1826, grad_fn=<MseLossBackward>)\n",
      "epoch 653,loss 1.1825854778289795\n",
      "tensor(1.1824, grad_fn=<MseLossBackward>)\n",
      "epoch 654,loss 1.1824394464492798\n",
      "tensor(1.1823, grad_fn=<MseLossBackward>)\n",
      "epoch 655,loss 1.1822947263717651\n",
      "tensor(1.1822, grad_fn=<MseLossBackward>)\n",
      "epoch 656,loss 1.1821516752243042\n",
      "tensor(1.1820, grad_fn=<MseLossBackward>)\n",
      "epoch 657,loss 1.1820098161697388\n",
      "tensor(1.1819, grad_fn=<MseLossBackward>)\n",
      "epoch 658,loss 1.181869387626648\n",
      "tensor(1.1817, grad_fn=<MseLossBackward>)\n",
      "epoch 659,loss 1.1817303895950317\n",
      "tensor(1.1816, grad_fn=<MseLossBackward>)\n",
      "epoch 660,loss 1.1815929412841797\n",
      "tensor(1.1815, grad_fn=<MseLossBackward>)\n",
      "epoch 661,loss 1.1814565658569336\n",
      "tensor(1.1813, grad_fn=<MseLossBackward>)\n",
      "epoch 662,loss 1.1813217401504517\n",
      "tensor(1.1812, grad_fn=<MseLossBackward>)\n",
      "epoch 663,loss 1.1811879873275757\n",
      "tensor(1.1811, grad_fn=<MseLossBackward>)\n",
      "epoch 664,loss 1.181056022644043\n",
      "tensor(1.1809, grad_fn=<MseLossBackward>)\n",
      "epoch 665,loss 1.1809252500534058\n",
      "tensor(1.1808, grad_fn=<MseLossBackward>)\n",
      "epoch 666,loss 1.1807953119277954\n",
      "tensor(1.1807, grad_fn=<MseLossBackward>)\n",
      "epoch 667,loss 1.1806669235229492\n",
      "tensor(1.1805, grad_fn=<MseLossBackward>)\n",
      "epoch 668,loss 1.1805402040481567\n",
      "tensor(1.1804, grad_fn=<MseLossBackward>)\n",
      "epoch 669,loss 1.180414080619812\n",
      "tensor(1.1803, grad_fn=<MseLossBackward>)\n",
      "epoch 670,loss 1.180289626121521\n",
      "tensor(1.1802, grad_fn=<MseLossBackward>)\n",
      "epoch 671,loss 1.180166244506836\n",
      "tensor(1.1800, grad_fn=<MseLossBackward>)\n",
      "epoch 672,loss 1.1800440549850464\n",
      "tensor(1.1799, grad_fn=<MseLossBackward>)\n",
      "epoch 673,loss 1.1799230575561523\n",
      "tensor(1.1798, grad_fn=<MseLossBackward>)\n",
      "epoch 674,loss 1.1798032522201538\n",
      "tensor(1.1797, grad_fn=<MseLossBackward>)\n",
      "epoch 675,loss 1.1796849966049194\n",
      "tensor(1.1796, grad_fn=<MseLossBackward>)\n",
      "epoch 676,loss 1.1795673370361328\n",
      "tensor(1.1795, grad_fn=<MseLossBackward>)\n",
      "epoch 677,loss 1.1794513463974\n",
      "tensor(1.1793, grad_fn=<MseLossBackward>)\n",
      "epoch 678,loss 1.179336428642273\n",
      "tensor(1.1792, grad_fn=<MseLossBackward>)\n",
      "epoch 679,loss 1.1792224645614624\n",
      "tensor(1.1791, grad_fn=<MseLossBackward>)\n",
      "epoch 680,loss 1.1791095733642578\n",
      "tensor(1.1790, grad_fn=<MseLossBackward>)\n",
      "epoch 681,loss 1.1789978742599487\n",
      "tensor(1.1789, grad_fn=<MseLossBackward>)\n",
      "epoch 682,loss 1.1788872480392456\n",
      "tensor(1.1788, grad_fn=<MseLossBackward>)\n",
      "epoch 683,loss 1.1787779331207275\n",
      "tensor(1.1787, grad_fn=<MseLossBackward>)\n",
      "epoch 684,loss 1.1786694526672363\n",
      "tensor(1.1786, grad_fn=<MseLossBackward>)\n",
      "epoch 685,loss 1.178562045097351\n",
      "tensor(1.1785, grad_fn=<MseLossBackward>)\n",
      "epoch 686,loss 1.1784560680389404\n",
      "tensor(1.1784, grad_fn=<MseLossBackward>)\n",
      "epoch 687,loss 1.1783509254455566\n",
      "tensor(1.1782, grad_fn=<MseLossBackward>)\n",
      "epoch 688,loss 1.1782466173171997\n",
      "tensor(1.1781, grad_fn=<MseLossBackward>)\n",
      "epoch 689,loss 1.1781435012817383\n",
      "tensor(1.1780, grad_fn=<MseLossBackward>)\n",
      "epoch 690,loss 1.1780411005020142\n",
      "tensor(1.1779, grad_fn=<MseLossBackward>)\n",
      "epoch 691,loss 1.1779402494430542\n",
      "tensor(1.1778, grad_fn=<MseLossBackward>)\n",
      "epoch 692,loss 1.1778403520584106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1777, grad_fn=<MseLossBackward>)\n",
      "epoch 693,loss 1.1777410507202148\n",
      "tensor(1.1776, grad_fn=<MseLossBackward>)\n",
      "epoch 694,loss 1.1776429414749146\n",
      "tensor(1.1775, grad_fn=<MseLossBackward>)\n",
      "epoch 695,loss 1.1775460243225098\n",
      "tensor(1.1774, grad_fn=<MseLossBackward>)\n",
      "epoch 696,loss 1.1774497032165527\n",
      "tensor(1.1774, grad_fn=<MseLossBackward>)\n",
      "epoch 697,loss 1.1773545742034912\n",
      "tensor(1.1773, grad_fn=<MseLossBackward>)\n",
      "epoch 698,loss 1.1772602796554565\n",
      "tensor(1.1772, grad_fn=<MseLossBackward>)\n",
      "epoch 699,loss 1.1771669387817383\n",
      "tensor(1.1771, grad_fn=<MseLossBackward>)\n",
      "epoch 700,loss 1.1770745515823364\n",
      "tensor(1.1770, grad_fn=<MseLossBackward>)\n",
      "epoch 701,loss 1.1769828796386719\n",
      "tensor(1.1769, grad_fn=<MseLossBackward>)\n",
      "epoch 702,loss 1.1768923997879028\n",
      "tensor(1.1768, grad_fn=<MseLossBackward>)\n",
      "epoch 703,loss 1.176802635192871\n",
      "tensor(1.1767, grad_fn=<MseLossBackward>)\n",
      "epoch 704,loss 1.1767140626907349\n",
      "tensor(1.1766, grad_fn=<MseLossBackward>)\n",
      "epoch 705,loss 1.1766258478164673\n",
      "tensor(1.1765, grad_fn=<MseLossBackward>)\n",
      "epoch 706,loss 1.1765391826629639\n",
      "tensor(1.1765, grad_fn=<MseLossBackward>)\n",
      "epoch 707,loss 1.176452875137329\n",
      "tensor(1.1764, grad_fn=<MseLossBackward>)\n",
      "epoch 708,loss 1.1763672828674316\n",
      "tensor(1.1763, grad_fn=<MseLossBackward>)\n",
      "epoch 709,loss 1.1762827634811401\n",
      "tensor(1.1762, grad_fn=<MseLossBackward>)\n",
      "epoch 710,loss 1.1761994361877441\n",
      "tensor(1.1761, grad_fn=<MseLossBackward>)\n",
      "epoch 711,loss 1.1761165857315063\n",
      "tensor(1.1760, grad_fn=<MseLossBackward>)\n",
      "epoch 712,loss 1.176034688949585\n",
      "tensor(1.1760, grad_fn=<MseLossBackward>)\n",
      "epoch 713,loss 1.1759530305862427\n",
      "tensor(1.1759, grad_fn=<MseLossBackward>)\n",
      "epoch 714,loss 1.175873041152954\n",
      "tensor(1.1758, grad_fn=<MseLossBackward>)\n",
      "epoch 715,loss 1.1757932901382446\n",
      "tensor(1.1757, grad_fn=<MseLossBackward>)\n",
      "epoch 716,loss 1.175714373588562\n",
      "tensor(1.1756, grad_fn=<MseLossBackward>)\n",
      "epoch 717,loss 1.1756364107131958\n",
      "tensor(1.1756, grad_fn=<MseLossBackward>)\n",
      "epoch 718,loss 1.175559163093567\n",
      "tensor(1.1755, grad_fn=<MseLossBackward>)\n",
      "epoch 719,loss 1.1754827499389648\n",
      "tensor(1.1754, grad_fn=<MseLossBackward>)\n",
      "epoch 720,loss 1.1754070520401\n",
      "tensor(1.1753, grad_fn=<MseLossBackward>)\n",
      "epoch 721,loss 1.1753320693969727\n",
      "tensor(1.1753, grad_fn=<MseLossBackward>)\n",
      "epoch 722,loss 1.1752574443817139\n",
      "tensor(1.1752, grad_fn=<MseLossBackward>)\n",
      "epoch 723,loss 1.1751842498779297\n",
      "tensor(1.1751, grad_fn=<MseLossBackward>)\n",
      "epoch 724,loss 1.1751112937927246\n",
      "tensor(1.1750, grad_fn=<MseLossBackward>)\n",
      "epoch 725,loss 1.1750396490097046\n",
      "tensor(1.1750, grad_fn=<MseLossBackward>)\n",
      "epoch 726,loss 1.1749680042266846\n",
      "tensor(1.1749, grad_fn=<MseLossBackward>)\n",
      "epoch 727,loss 1.1748971939086914\n",
      "tensor(1.1748, grad_fn=<MseLossBackward>)\n",
      "epoch 728,loss 1.174827218055725\n",
      "tensor(1.1748, grad_fn=<MseLossBackward>)\n",
      "epoch 729,loss 1.1747584342956543\n",
      "tensor(1.1747, grad_fn=<MseLossBackward>)\n",
      "epoch 730,loss 1.174689531326294\n",
      "tensor(1.1746, grad_fn=<MseLossBackward>)\n",
      "epoch 731,loss 1.174621820449829\n",
      "tensor(1.1746, grad_fn=<MseLossBackward>)\n",
      "epoch 732,loss 1.174554467201233\n",
      "tensor(1.1745, grad_fn=<MseLossBackward>)\n",
      "epoch 733,loss 1.1744877099990845\n",
      "tensor(1.1744, grad_fn=<MseLossBackward>)\n",
      "epoch 734,loss 1.174422264099121\n",
      "tensor(1.1744, grad_fn=<MseLossBackward>)\n",
      "epoch 735,loss 1.1743566989898682\n",
      "tensor(1.1743, grad_fn=<MseLossBackward>)\n",
      "epoch 736,loss 1.1742923259735107\n",
      "tensor(1.1742, grad_fn=<MseLossBackward>)\n",
      "epoch 737,loss 1.174228549003601\n",
      "tensor(1.1742, grad_fn=<MseLossBackward>)\n",
      "epoch 738,loss 1.174164891242981\n",
      "tensor(1.1741, grad_fn=<MseLossBackward>)\n",
      "epoch 739,loss 1.1741024255752563\n",
      "tensor(1.1740, grad_fn=<MseLossBackward>)\n",
      "epoch 740,loss 1.1740405559539795\n",
      "tensor(1.1740, grad_fn=<MseLossBackward>)\n",
      "epoch 741,loss 1.1739786863327026\n",
      "tensor(1.1739, grad_fn=<MseLossBackward>)\n",
      "epoch 742,loss 1.1739177703857422\n",
      "tensor(1.1739, grad_fn=<MseLossBackward>)\n",
      "epoch 743,loss 1.173857569694519\n",
      "tensor(1.1738, grad_fn=<MseLossBackward>)\n",
      "epoch 744,loss 1.1737980842590332\n",
      "tensor(1.1737, grad_fn=<MseLossBackward>)\n",
      "epoch 745,loss 1.173738956451416\n",
      "tensor(1.1737, grad_fn=<MseLossBackward>)\n",
      "epoch 746,loss 1.1736804246902466\n",
      "tensor(1.1736, grad_fn=<MseLossBackward>)\n",
      "epoch 747,loss 1.173622488975525\n",
      "tensor(1.1736, grad_fn=<MseLossBackward>)\n",
      "epoch 748,loss 1.1735652685165405\n",
      "tensor(1.1735, grad_fn=<MseLossBackward>)\n",
      "epoch 749,loss 1.173508644104004\n",
      "tensor(1.1735, grad_fn=<MseLossBackward>)\n",
      "epoch 750,loss 1.1734522581100464\n",
      "tensor(1.1734, grad_fn=<MseLossBackward>)\n",
      "epoch 751,loss 1.1733967065811157\n",
      "tensor(1.1733, grad_fn=<MseLossBackward>)\n",
      "epoch 752,loss 1.1733416318893433\n",
      "tensor(1.1733, grad_fn=<MseLossBackward>)\n",
      "epoch 753,loss 1.1732869148254395\n",
      "tensor(1.1732, grad_fn=<MseLossBackward>)\n",
      "epoch 754,loss 1.173232913017273\n",
      "tensor(1.1732, grad_fn=<MseLossBackward>)\n",
      "epoch 755,loss 1.1731793880462646\n",
      "tensor(1.1731, grad_fn=<MseLossBackward>)\n",
      "epoch 756,loss 1.1731266975402832\n",
      "tensor(1.1731, grad_fn=<MseLossBackward>)\n",
      "epoch 757,loss 1.1730740070343018\n",
      "tensor(1.1730, grad_fn=<MseLossBackward>)\n",
      "epoch 758,loss 1.1730225086212158\n",
      "tensor(1.1730, grad_fn=<MseLossBackward>)\n",
      "epoch 759,loss 1.1729707717895508\n",
      "tensor(1.1729, grad_fn=<MseLossBackward>)\n",
      "epoch 760,loss 1.1729203462600708\n",
      "tensor(1.1729, grad_fn=<MseLossBackward>)\n",
      "epoch 761,loss 1.1728699207305908\n",
      "tensor(1.1728, grad_fn=<MseLossBackward>)\n",
      "epoch 762,loss 1.1728198528289795\n",
      "tensor(1.1728, grad_fn=<MseLossBackward>)\n",
      "epoch 763,loss 1.1727702617645264\n",
      "tensor(1.1727, grad_fn=<MseLossBackward>)\n",
      "epoch 764,loss 1.1727215051651\n",
      "tensor(1.1727, grad_fn=<MseLossBackward>)\n",
      "epoch 765,loss 1.172673225402832\n",
      "tensor(1.1726, grad_fn=<MseLossBackward>)\n",
      "epoch 766,loss 1.1726253032684326\n",
      "tensor(1.1726, grad_fn=<MseLossBackward>)\n",
      "epoch 767,loss 1.1725778579711914\n",
      "tensor(1.1725, grad_fn=<MseLossBackward>)\n",
      "epoch 768,loss 1.1725307703018188\n",
      "tensor(1.1725, grad_fn=<MseLossBackward>)\n",
      "epoch 769,loss 1.1724841594696045\n",
      "tensor(1.1724, grad_fn=<MseLossBackward>)\n",
      "epoch 770,loss 1.1724382638931274\n",
      "tensor(1.1724, grad_fn=<MseLossBackward>)\n",
      "epoch 771,loss 1.1723928451538086\n",
      "tensor(1.1723, grad_fn=<MseLossBackward>)\n",
      "epoch 772,loss 1.1723475456237793\n",
      "tensor(1.1723, grad_fn=<MseLossBackward>)\n",
      "epoch 773,loss 1.1723030805587769\n",
      "tensor(1.1723, grad_fn=<MseLossBackward>)\n",
      "epoch 774,loss 1.172258734703064\n",
      "tensor(1.1722, grad_fn=<MseLossBackward>)\n",
      "epoch 775,loss 1.1722149848937988\n",
      "tensor(1.1722, grad_fn=<MseLossBackward>)\n",
      "epoch 776,loss 1.1721713542938232\n",
      "tensor(1.1721, grad_fn=<MseLossBackward>)\n",
      "epoch 777,loss 1.1721285581588745\n",
      "tensor(1.1721, grad_fn=<MseLossBackward>)\n",
      "epoch 778,loss 1.1720861196517944\n",
      "tensor(1.1720, grad_fn=<MseLossBackward>)\n",
      "epoch 779,loss 1.1720439195632935\n",
      "tensor(1.1720, grad_fn=<MseLossBackward>)\n",
      "epoch 780,loss 1.1720020771026611\n",
      "tensor(1.1720, grad_fn=<MseLossBackward>)\n",
      "epoch 781,loss 1.1719609498977661\n",
      "tensor(1.1719, grad_fn=<MseLossBackward>)\n",
      "epoch 782,loss 1.171919345855713\n",
      "tensor(1.1719, grad_fn=<MseLossBackward>)\n",
      "epoch 783,loss 1.171879529953003\n",
      "tensor(1.1718, grad_fn=<MseLossBackward>)\n",
      "epoch 784,loss 1.1718395948410034\n",
      "tensor(1.1718, grad_fn=<MseLossBackward>)\n",
      "epoch 785,loss 1.1718000173568726\n",
      "tensor(1.1718, grad_fn=<MseLossBackward>)\n",
      "epoch 786,loss 1.1717606782913208\n",
      "tensor(1.1717, grad_fn=<MseLossBackward>)\n",
      "epoch 787,loss 1.1717215776443481\n",
      "tensor(1.1717, grad_fn=<MseLossBackward>)\n",
      "epoch 788,loss 1.1716831922531128\n",
      "tensor(1.1716, grad_fn=<MseLossBackward>)\n",
      "epoch 789,loss 1.171645164489746\n",
      "tensor(1.1716, grad_fn=<MseLossBackward>)\n",
      "epoch 790,loss 1.171607494354248\n",
      "tensor(1.1716, grad_fn=<MseLossBackward>)\n",
      "epoch 791,loss 1.171570062637329\n",
      "tensor(1.1715, grad_fn=<MseLossBackward>)\n",
      "epoch 792,loss 1.1715331077575684\n",
      "tensor(1.1715, grad_fn=<MseLossBackward>)\n",
      "epoch 793,loss 1.1714963912963867\n",
      "tensor(1.1715, grad_fn=<MseLossBackward>)\n",
      "epoch 794,loss 1.1714599132537842\n",
      "tensor(1.1714, grad_fn=<MseLossBackward>)\n",
      "epoch 795,loss 1.171424150466919\n",
      "tensor(1.1714, grad_fn=<MseLossBackward>)\n",
      "epoch 796,loss 1.1713887453079224\n",
      "tensor(1.1714, grad_fn=<MseLossBackward>)\n",
      "epoch 797,loss 1.1713535785675049\n",
      "tensor(1.1713, grad_fn=<MseLossBackward>)\n",
      "epoch 798,loss 1.171318769454956\n",
      "tensor(1.1713, grad_fn=<MseLossBackward>)\n",
      "epoch 799,loss 1.1712843179702759\n",
      "tensor(1.1712, grad_fn=<MseLossBackward>)\n",
      "epoch 800,loss 1.1712498664855957\n",
      "tensor(1.1712, grad_fn=<MseLossBackward>)\n",
      "epoch 801,loss 1.1712160110473633\n",
      "tensor(1.1712, grad_fn=<MseLossBackward>)\n",
      "epoch 802,loss 1.1711827516555786\n",
      "tensor(1.1711, grad_fn=<MseLossBackward>)\n",
      "epoch 803,loss 1.1711493730545044\n",
      "tensor(1.1711, grad_fn=<MseLossBackward>)\n",
      "epoch 804,loss 1.171116590499878\n",
      "tensor(1.1711, grad_fn=<MseLossBackward>)\n",
      "epoch 805,loss 1.1710841655731201\n",
      "tensor(1.1711, grad_fn=<MseLossBackward>)\n",
      "epoch 806,loss 1.1710517406463623\n",
      "tensor(1.1710, grad_fn=<MseLossBackward>)\n",
      "epoch 807,loss 1.1710201501846313\n",
      "tensor(1.1710, grad_fn=<MseLossBackward>)\n",
      "epoch 808,loss 1.1709884405136108\n",
      "tensor(1.1710, grad_fn=<MseLossBackward>)\n",
      "epoch 809,loss 1.1709572076797485\n",
      "tensor(1.1709, grad_fn=<MseLossBackward>)\n",
      "epoch 810,loss 1.170926570892334\n",
      "tensor(1.1709, grad_fn=<MseLossBackward>)\n",
      "epoch 811,loss 1.1708958148956299\n",
      "tensor(1.1709, grad_fn=<MseLossBackward>)\n",
      "epoch 812,loss 1.170865535736084\n",
      "tensor(1.1708, grad_fn=<MseLossBackward>)\n",
      "epoch 813,loss 1.1708354949951172\n",
      "tensor(1.1708, grad_fn=<MseLossBackward>)\n",
      "epoch 814,loss 1.170805811882019\n",
      "tensor(1.1708, grad_fn=<MseLossBackward>)\n",
      "epoch 815,loss 1.1707764863967896\n",
      "tensor(1.1707, grad_fn=<MseLossBackward>)\n",
      "epoch 816,loss 1.1707470417022705\n",
      "tensor(1.1707, grad_fn=<MseLossBackward>)\n",
      "epoch 817,loss 1.1707184314727783\n",
      "tensor(1.1707, grad_fn=<MseLossBackward>)\n",
      "epoch 818,loss 1.1706898212432861\n",
      "tensor(1.1707, grad_fn=<MseLossBackward>)\n",
      "epoch 819,loss 1.170661449432373\n",
      "tensor(1.1706, grad_fn=<MseLossBackward>)\n",
      "epoch 820,loss 1.1706336736679077\n",
      "tensor(1.1706, grad_fn=<MseLossBackward>)\n",
      "epoch 821,loss 1.1706055402755737\n",
      "tensor(1.1706, grad_fn=<MseLossBackward>)\n",
      "epoch 822,loss 1.1705783605575562\n",
      "tensor(1.1706, grad_fn=<MseLossBackward>)\n",
      "epoch 823,loss 1.1705514192581177\n",
      "tensor(1.1705, grad_fn=<MseLossBackward>)\n",
      "epoch 824,loss 1.1705242395401\n",
      "tensor(1.1705, grad_fn=<MseLossBackward>)\n",
      "epoch 825,loss 1.1704975366592407\n",
      "tensor(1.1705, grad_fn=<MseLossBackward>)\n",
      "epoch 826,loss 1.1704710721969604\n",
      "tensor(1.1704, grad_fn=<MseLossBackward>)\n",
      "epoch 827,loss 1.1704449653625488\n",
      "tensor(1.1704, grad_fn=<MseLossBackward>)\n",
      "epoch 828,loss 1.1704193353652954\n",
      "tensor(1.1704, grad_fn=<MseLossBackward>)\n",
      "epoch 829,loss 1.170393705368042\n",
      "tensor(1.1704, grad_fn=<MseLossBackward>)\n",
      "epoch 830,loss 1.1703684329986572\n",
      "tensor(1.1703, grad_fn=<MseLossBackward>)\n",
      "epoch 831,loss 1.1703431606292725\n",
      "tensor(1.1703, grad_fn=<MseLossBackward>)\n",
      "epoch 832,loss 1.1703184843063354\n",
      "tensor(1.1703, grad_fn=<MseLossBackward>)\n",
      "epoch 833,loss 1.170293927192688\n",
      "tensor(1.1703, grad_fn=<MseLossBackward>)\n",
      "epoch 834,loss 1.1702693700790405\n",
      "tensor(1.1702, grad_fn=<MseLossBackward>)\n",
      "epoch 835,loss 1.1702454090118408\n",
      "tensor(1.1702, grad_fn=<MseLossBackward>)\n",
      "epoch 836,loss 1.1702213287353516\n",
      "tensor(1.1702, grad_fn=<MseLossBackward>)\n",
      "epoch 837,loss 1.1701979637145996\n",
      "tensor(1.1702, grad_fn=<MseLossBackward>)\n",
      "epoch 838,loss 1.1701743602752686\n",
      "tensor(1.1702, grad_fn=<MseLossBackward>)\n",
      "epoch 839,loss 1.1701511144638062\n",
      "tensor(1.1701, grad_fn=<MseLossBackward>)\n",
      "epoch 840,loss 1.170128345489502\n",
      "tensor(1.1701, grad_fn=<MseLossBackward>)\n",
      "epoch 841,loss 1.1701055765151978\n",
      "tensor(1.1701, grad_fn=<MseLossBackward>)\n",
      "epoch 842,loss 1.1700830459594727\n",
      "tensor(1.1701, grad_fn=<MseLossBackward>)\n",
      "epoch 843,loss 1.1700607538223267\n",
      "tensor(1.1700, grad_fn=<MseLossBackward>)\n",
      "epoch 844,loss 1.1700385808944702\n",
      "tensor(1.1700, grad_fn=<MseLossBackward>)\n",
      "epoch 845,loss 1.1700170040130615\n",
      "tensor(1.1700, grad_fn=<MseLossBackward>)\n",
      "epoch 846,loss 1.1699954271316528\n",
      "tensor(1.1700, grad_fn=<MseLossBackward>)\n",
      "epoch 847,loss 1.1699738502502441\n",
      "tensor(1.1700, grad_fn=<MseLossBackward>)\n",
      "epoch 848,loss 1.169952630996704\n",
      "tensor(1.1699, grad_fn=<MseLossBackward>)\n",
      "epoch 849,loss 1.1699318885803223\n",
      "tensor(1.1699, grad_fn=<MseLossBackward>)\n",
      "epoch 850,loss 1.1699107885360718\n",
      "tensor(1.1699, grad_fn=<MseLossBackward>)\n",
      "epoch 851,loss 1.1698904037475586\n",
      "tensor(1.1699, grad_fn=<MseLossBackward>)\n",
      "epoch 852,loss 1.1698698997497559\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 853,loss 1.1698496341705322\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 854,loss 1.1698299646377563\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 855,loss 1.1698102951049805\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 856,loss 1.1697903871536255\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 857,loss 1.1697711944580078\n",
      "tensor(1.1698, grad_fn=<MseLossBackward>)\n",
      "epoch 858,loss 1.1697521209716797\n",
      "tensor(1.1697, grad_fn=<MseLossBackward>)\n",
      "epoch 859,loss 1.1697328090667725\n",
      "tensor(1.1697, grad_fn=<MseLossBackward>)\n",
      "epoch 860,loss 1.169714093208313\n",
      "tensor(1.1697, grad_fn=<MseLossBackward>)\n",
      "epoch 861,loss 1.1696953773498535\n",
      "tensor(1.1697, grad_fn=<MseLossBackward>)\n",
      "epoch 862,loss 1.1696772575378418\n",
      "tensor(1.1697, grad_fn=<MseLossBackward>)\n",
      "epoch 863,loss 1.16965913772583\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 864,loss 1.1696407794952393\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 865,loss 1.1696230173110962\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 866,loss 1.1696054935455322\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 867,loss 1.1695873737335205\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 868,loss 1.1695702075958252\n",
      "tensor(1.1696, grad_fn=<MseLossBackward>)\n",
      "epoch 869,loss 1.169553279876709\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 870,loss 1.1695361137390137\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 871,loss 1.169519066810608\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 872,loss 1.1695027351379395\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 873,loss 1.1694860458374023\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 874,loss 1.1694698333740234\n",
      "tensor(1.1695, grad_fn=<MseLossBackward>)\n",
      "epoch 875,loss 1.1694536209106445\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 876,loss 1.1694375276565552\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 877,loss 1.169421672821045\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 878,loss 1.1694060564041138\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 879,loss 1.1693906784057617\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 880,loss 1.1693750619888306\n",
      "tensor(1.1694, grad_fn=<MseLossBackward>)\n",
      "epoch 881,loss 1.1693599224090576\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 882,loss 1.1693446636199951\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 883,loss 1.1693296432495117\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 884,loss 1.1693147420883179\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 885,loss 1.1693003177642822\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 886,loss 1.1692860126495361\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 887,loss 1.1692711114883423\n",
      "tensor(1.1693, grad_fn=<MseLossBackward>)\n",
      "epoch 888,loss 1.1692570447921753\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 889,loss 1.1692428588867188\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 890,loss 1.1692290306091309\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 891,loss 1.169215202331543\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 892,loss 1.169201374053955\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 893,loss 1.1691879034042358\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 894,loss 1.1691745519638062\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "epoch 895,loss 1.1691614389419556\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 896,loss 1.1691482067108154\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 897,loss 1.1691352128982544\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 898,loss 1.169122576713562\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 899,loss 1.1691094636917114\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 900,loss 1.1690967082977295\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 901,loss 1.1690844297409058\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 902,loss 1.169072151184082\n",
      "tensor(1.1691, grad_fn=<MseLossBackward>)\n",
      "epoch 903,loss 1.1690597534179688\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 904,loss 1.1690475940704346\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 905,loss 1.1690354347229004\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 906,loss 1.169023871421814\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 907,loss 1.169012188911438\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 908,loss 1.169000267982483\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 909,loss 1.168988585472107\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 910,loss 1.1689772605895996\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 911,loss 1.1689659357070923\n",
      "tensor(1.1690, grad_fn=<MseLossBackward>)\n",
      "epoch 912,loss 1.1689547300338745\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 913,loss 1.1689436435699463\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 914,loss 1.1689327955245972\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 915,loss 1.1689220666885376\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 916,loss 1.1689108610153198\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 917,loss 1.168900489807129\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 918,loss 1.168890118598938\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 919,loss 1.168879508972168\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 920,loss 1.1688692569732666\n",
      "tensor(1.1689, grad_fn=<MseLossBackward>)\n",
      "epoch 921,loss 1.1688590049743652\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 922,loss 1.1688486337661743\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 923,loss 1.1688388586044312\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 924,loss 1.1688288450241089\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 925,loss 1.1688188314437866\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 926,loss 1.168809175491333\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 927,loss 1.1687994003295898\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 928,loss 1.1687901020050049\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 929,loss 1.1687804460525513\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 930,loss 1.1687710285186768\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 931,loss 1.1687617301940918\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "epoch 932,loss 1.1687525510787964\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 933,loss 1.168743371963501\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 934,loss 1.1687344312667847\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 935,loss 1.1687254905700684\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 936,loss 1.1687170267105103\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 937,loss 1.168708324432373\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 938,loss 1.1686993837356567\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 939,loss 1.1686910390853882\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 940,loss 1.1686822175979614\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 941,loss 1.1686739921569824\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 942,loss 1.1686656475067139\n",
      "tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "epoch 943,loss 1.168657660484314\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 944,loss 1.1686493158340454\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 945,loss 1.1686409711837769\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 946,loss 1.168633222579956\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 947,loss 1.1686253547668457\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 948,loss 1.168617606163025\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 949,loss 1.1686094999313354\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 950,loss 1.1686019897460938\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 951,loss 1.1685943603515625\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 952,loss 1.1685870885849\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 953,loss 1.1685794591903687\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 954,loss 1.1685718297958374\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 955,loss 1.168564796447754\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 956,loss 1.1685576438903809\n",
      "tensor(1.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 957,loss 1.1685503721237183\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 958,loss 1.1685435771942139\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 959,loss 1.1685361862182617\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 960,loss 1.1685295104980469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 961,loss 1.1685221195220947\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 962,loss 1.168515682220459\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 963,loss 1.168508768081665\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 964,loss 1.1685023307800293\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 965,loss 1.1684954166412354\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 966,loss 1.1684889793395996\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 967,loss 1.1684825420379639\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 968,loss 1.1684762239456177\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 969,loss 1.168470025062561\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 970,loss 1.1684634685516357\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 971,loss 1.1684571504592896\n",
      "tensor(1.1685, grad_fn=<MseLossBackward>)\n",
      "epoch 972,loss 1.168450951576233\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 973,loss 1.1684448719024658\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 974,loss 1.1684390306472778\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 975,loss 1.1684329509735107\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 976,loss 1.1684269905090332\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 977,loss 1.1684212684631348\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 978,loss 1.1684153079986572\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 979,loss 1.1684094667434692\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 980,loss 1.1684041023254395\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 981,loss 1.1683980226516724\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 982,loss 1.1683927774429321\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 983,loss 1.1683871746063232\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 984,loss 1.168381690979004\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 985,loss 1.1683762073516846\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 986,loss 1.1683709621429443\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 987,loss 1.1683655977249146\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 988,loss 1.168360710144043\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 989,loss 1.1683552265167236\n",
      "tensor(1.1684, grad_fn=<MseLossBackward>)\n",
      "epoch 990,loss 1.168350100517273\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 991,loss 1.1683446168899536\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 992,loss 1.1683396100997925\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 993,loss 1.1683348417282104\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 994,loss 1.1683297157287598\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 995,loss 1.1683250665664673\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 996,loss 1.1683199405670166\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 997,loss 1.1683154106140137\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 998,loss 1.1683106422424316\n",
      "tensor(1.1683, grad_fn=<MseLossBackward>)\n",
      "epoch 999,loss 1.16830575466156\n"
     ]
    }
   ],
   "source": [
    "# Reshape data for pytorch\n",
    "t_train=T.reshape(-1,1)\n",
    "x_train=X.reshape(-1,1)\n",
    "\n",
    "t_train=np.float32(t_train)\n",
    "x_train=np.float32(x_train)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and targets to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs=Variable(torch.from_numpy(t_train).cuda())\n",
    "        targets=Variable(torch.from_numpy(x_train).cuda())\n",
    "    else:\n",
    "        inputs=Variable(torch.from_numpy(t_train))\n",
    "        targets=Variable(torch.from_numpy(x_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs=model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss=criterion(outputs,targets)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {},loss {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[3.0071]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([-5.0308], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve parameters \n",
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted targets are available already from above also \n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted=model(Variable(torch.from_numpy(t_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted=model(Variable(torch.from_numpy(t_train))).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hUVf7H8feZmUwmjUBCgFCD0iEIGJAiglIURIptLatYdtFVLOv6cykqigiuq66uuioqq+siuiKgiKAUAQFBiNI7SgmphJA+SWbm/P44ExIkQCCTTMr39Tw8987NmTvngn44nDlFaa0RQghRc1n8XQEhhBAVI0EuhBA1nAS5EELUcBLkQghRw0mQCyFEDWfzx4c2bNhQx8TE+OOjhRCixoqPjz+mtY767XW/BHlMTAybNm3yx0cLIUSNpZQ6VNZ16VoRQogaToJcCCFqOAlyIYSo4fzSR16WoqIiEhIScDqd/q6K3zkcDpo3b05AQIC/qyKEqAGqTZAnJCQQFhZGTEwMSil/V8dvtNakp6eTkJBA69at/V0dIUQNUG26VpxOJ5GRkXU6xAGUUkRGRsq/TIQQ5VbuIFdKtVBKfaeU2qWU2qGUesR7/Rml1FGl1Gbvr+EXWpm6HuLF5PdBCHE+zqdF7gL+orXuCPQGHlRKdfL+7B9a627eX1/7vJZCCFHdZH0OSfeCO7tcxXMLXPxtyW6O5RT4vCrlDnKtdZLW+ifveTawC2jm8xr52fz581FKsXv37rOW++CDD0hMTLzgz1m5ciUjRoy44PcLIfxIa0j9M2TOgmPPnLP4it0pDP3Hat5aeYDnF+3yeXUuqI9cKRUDdAc2eC+NV0ptVUrNUko1OMN7ximlNimlNqWlpV1QZavCnDlzuPzyy/nkk0/OWq6iQS6EqMEKd4DriDnPeA0KdpRZLDXLyYMf/8Q9H2zi6Il8ujQN4p5+vh/EcN5BrpQKBT4HHtVaZwFvARcD3YAk4OWy3qe1nqm1jtNax0VFnbZUQLWQk5PD2rVref/9908J8hdffJHY2FguueQSJkyYwNy5c9m0aRO333473bp1Iz8/n5iYGI4dOwbApk2bGDhwIAA//vgjffv2pXv37vTt25c9e/b449GEEL6U4+1BVnbADSkPm1a6l8ejmb3hEINeWcWirUkEBVh5cohmweDhxAa95/PqnNfwQ6VUACbEZ2ut5wForVNK/fxd4KsK12p3JX3Z1+Hs29otWLCAa665hnbt2hEREcFPP/1ESkoKCxYsYMOGDQQHB3P8+HEiIiJ44403eOmll4iLizv7R3bowOrVq7HZbCxbtoxJkybx+eef+/KphBBVLXexOTZ6DdImQ94KyJ4L9W5iX0o2E+dtY9OhDACubB/FcyOiaH6iJ7gLAI/Pq1PuIFdmKMX7wC6t9SulrkdrrZO8L8cA231bxaozZ84cHn30UQBuueUW5syZg8fj4e677yY4OBiAiIiI87pnZmYmY8eOZd++fSilKCoq8nm9hRAVpDUcexpsjaHB+LOXdWdC3hrACvVuAWWB5PtwJj7Bm6tDePsHKHJrosICmXJdJ67t0giVMATc6RByNUT8xefVP58WeT/gDmCbUmqz99ok4FalVDdAAweB+ypcq3O0nCtDeno6K1asYPv27SilcLvdKKW44YYbyjUc0Gaz4fGYv2lLjwF/6qmnuPLKK5k/fz4HDx482eUihKhGCn6G9GmADcLvAUvwmcvmLgNcENQfrPUh/F7W7VjK5FVD+DXLZNetnfcwYWgU4ZHd4PgMyFsJ1sYQ/aEJfh8rd5BrrdcAZSVarRhuOHfuXO68807eeeedk9cGDBhAREQEs2bN4rbbbjulayUsLIzs7JJhRzExMcTHxzNs2LBTuk4yMzNp1swM7vnggw+q7HmEEOche773xAXOjRA84Mxlc72RFzqcjNxCnv96F3Pj7wKgTf1UZvR5iZ6Nd0IGkGHBtHGBph+ZFn8lqDYzO/1tzpw5jBkz5pRrN9xwA4mJiYwcOZK4uDi6devGSy+9BMBdd93F/ffff/LLzilTpvDII4/Qv39/rFbryXs88cQTTJw4kX79+uF2u6v0mYQQ5ZSzoOQ8f92Zy2kNuYvRGubtH8CgV1YxNz4Bu9XCY0PasejxO+nZ92to/DqEDAWsgIaICRAypNKqr7Su+m6MuLg4/duNJXbt2kXHjh2rvC7Vlfx+CFFFCvfDL21LXoeMgBYLyy7r3MyhrcOYvP4x1iSa+ZC9L4pg+phYLooKPb28OwsK94LjUvDBjG2lVLzW+rQRFtVm0SwhhPCL4m6VoH6Qv9a0yLU+LXiL3B7eXb6J19a9SYE7kPrBAUwa3pGbLm1+5u/RrPUg6Owj23xBglwIUbfleIO8waNQ9Cu4Ek0rOrD9ySI/Hc5g0rxt7E6OBmBMFzdPjh5MZGigP2p8GglyIUTd5UqC/PWgAiH0Gsjua8aD56+FwPZkO4v4+zd7+Gj9IbSGlmFJTOv7EVcMWASW6hHiIEEuhKjLsr8EtPli0hIKQcVBvo4lCcOZ8uV2UrIKsFkUf+y2loc7v0xQ9BRTthqRIBdC1F3F3Sqho80xqC9JuZE8/V1blh6KB6Bbi/rMGJZDx8LpYI2E+ueYMOQHEuRCiLrJnQW5KwALhF6H26P56KdI/r74bXJdQYQGWnnimg7c3qsl1iP9zHsingBrmF+rXRYZR16KUoo77rjj5GuXy0VUVNR5LzdbegGtipQRQlSigp+BInD0YGdqINe/tY5nFu4h1xXE1S3Xsew+N3f2icGa/w0414M1Cho86O9al0la5KWEhISwfft28vPzCQoKYunSpSdnZQohahnnVvJdgby68Tbe+3kNbo+mST0Hzw78iasjpkPAJMgNhhRvV0rEE2AJ8W+dz0Ba5L8xbNgwFi1aBJjZnrfeeuvJnx0/fpzRo0fTtWtXevfuzdatWwGzTsvQoUPp3r079913H6UnWf33v/+lV69edOvWjfvuu09mdwpRTazem8TQBW/yTnw7PFpzV98Ylj52BVfHeocdHn8VjlwFRb+AvRM0+JN/K3wW1bJFHjNhUaXc9+AL156zzC233MLUqVMZMWIEW7du5Z577uH7778HYMqUKXTv3p0FCxawYsUK7rzzTjZv3syzzz7L5ZdfztNPP82iRYuYOXMmYGZnfvrpp6xdu5aAgAAeeOABZs+ezZ133lkpzydEnVf4CwS0AmU9Y5FjOQU899VOvths+r07NLbwwo196Naivilg62OOOg9UMERONCsWWoIqu/YXrFoGuT917dqVgwcPMmfOHIYPP3Uf6TVr1pxcEOuqq64iPT2dzMxMVq9ezbx58wC49tpradDAbJK0fPly4uPj6dmzJwD5+fk0atSoCp9GiDok63+Q+DuIegkiT18qVmvNZ5sSeP7rXWTmF+GwFvBot4+5d9TbBNjrlxS0RUHU36DoCET+FQKaV+FDXJhqGeTlaTlXppEjR/L444+zcuVK0tPTT14va12a4qm5ZU3R1VozduxYZsyYUXmVFaKuyV1m1g5v8u9TZl+S+ZE55iw8LcgPpOUwad42Nvx6HID+FwfzfLd7adkgAOwNT/+MyCcqq/aVQvrIy3DPPffw9NNPExsbe8r1K664gtmzZwNm8+SGDRtSr169U64vXryYjAyzM8igQYOYO3cuqampgOljP3ToUBU+iRC10LGnIf8HyHil5JonH/KWm3PnRtAuAApcbl5bto9hr37Phl+PExli57VbuvGfm4/TMiwFArv64QF8r1q2yP2tefPmPPLII6ddf+aZZ7j77rvp2rUrwcHBfPjhh4DpO7/11lvp0aMHAwYMoGXLlgB06tSJadOmMXToUDweDwEBAbz55pu0atWqSp9HiFqj8KAJcYDsz6HxG6ACzMYNOt9c13lQsJ0fk1oycd5WDqTlAnBzXHMmDe9I/WA7pJmBCrUlyGUZ22pKfj9EnZH1GRQdgMgJ5y6b/iKk/bXkdYtvzPT65PFw4k1AkVkQzAs732fOFjON/qKoEKaPiaX3RZEl70sYY9Ygb/ox1LuVmkKWsRVCVD9aQ/L94DkOoWNO7fMuS9Yn5ui4FJzxkPUpBA+B3EVoDV+lTODZlZ045gwlwKr408A2PDDwYhwBvxnFUlC7WuTSRy6E8B93sglxgKJ9p/4s52tIugdc3hnQBXvMbExLODR511zLng8FWzhyPI+7l0/noSX9OOZsQK8mB1j8SH8eG9Lu9BB3Z5ux4coO9naV+3xVpFoFuT+6eaoj+X0QdUbBjpLzwgOn/uzYs5D5bzOkULsg+1NzPWwMOLqDvTMuVybvfjuboV/8i5UJXannsDGj79t8cvWjtIksNOWLEuBgTzj+uvdztpujvZPpX68Fqk3XisPhID09ncjIyHLtWl9baa1JT0/H4XD4uypCVL5Tgnx/ybnWULjbnOetgNTHIfcb8zrsFgC25t3NxK9t7DjeBoARHV08ff0gGh2fBvka8n+E0GGQ/gI4N4FzC4QMBmft6laBahTkzZs3JyEhgbS0NH9Xxe8cDgfNm1f/SQhCVFjpIC8qFeTuFPBkgXKAdkPGa+a6tSG5tit4eeFOPljXAY+GZiEpTOszkyuv/AasDsjvDflrzIYRjjjInFX8AZDyANi9gwgCTx1eXJNVmyAPCAigdevW/q6GEKIqFZ6ha6VwjzkGXgL174Hk+wBYlvYAT89dR2KmE6tFMS52NY/GvkpweF+whpv3OHqbo3M9ZCgzLDFogPmsvJWQv9Fbrg62yJVSLYD/AE0ADzBTa/2aUioC+BSIAQ4CN2utM3xfVSFEraI1FOwseV30q+kLV7aSILe3h/rjSEn/lWeXWvn6YC/ASWyzcGZcH0uX4EOQUgD1SpafJsi7Vkr+ejOyBSBqqlmHJflu0GZceV3tWnEBf9Fa/6SUCgPilVJLgbuA5VrrF5RSE4AJwF/Pch8hhDD7ZXpOgKUBWILBdRSKDoP9IjNCBfAEtGf2+kO8uHgg2QUugu1WHh/anrF9Y7BaFOj7IWSYWSirWEBTsLUA1xHz2tELgvqbX5mzIP97s7a4tbEfHrpylDvItdZJQJL3PFsptQtoBowCBnqLfQisRIJcCHEuxd0qgZ1NK9x11EwMsl8EhXvYk9GKSctiiU8wo0wGdWjE1NFdaFa/1CqESoE95vR7B/WGbG+QRzxhygE0eQsO9YfQUSXXaoEL6iNXSsUA3YENQGNvyKO1TlJKlbm8n1JqHDAOODmFXQhRhxWUCnLtAlZC4X6c9qt4fe3FvLPlj7i0haiwQJ4d2ZlhXZqUf0RbUG/I/gwC2kDY6JLrgZ2hTZIZQ16LnHeQK6VCgc+BR7XWWeX9jdVazwRmgpmif76fK4SoZYqD3N7JrI8CrNufzqSVqziYPhSA23o146/DOhMedJ7jvcPHmj7yBuNPX5vcEljRmlc75xXkSqkATIjP1lrP815OUUpFe1vj0UCqrysphKhBPPlmoo06R7yU6lo5np3B898/yucHugP5tK1/iBn95xHXb9mF1cEaCc3+d2HvrYHKPbNTmab3+8AurXWp9SP5EhjrPR8LfOG76gkhqg1PPqTPgKKjZy7jSob9zSDx92e/l3fEitYwb2cTBs8M4fMDg7Fbi3h8gItF1z1CXMva1f1Rmc6nRd4PuAPYppTa7L02CXgB+J9S6l7gMHCTb6sohKgWsj6CtElmZmSzOWWXyVkMngyzsqAuPHNftCuRgyeCmfzDJNYmHQSgT5MtTO/7Lq0v+iMcc4G9Q6U8Rm10PqNW1gBn6hAf5JvqCCGqreIp9LnfmNmWZe2LmfedOeoCcP4MQZedVqTI7WHmiq38c/UbFLgDaRAcwORrO3FD2D0oT2rJPeznWAlRnFRtZnYKIaq5Iu/uVp4MswtPUO9Tf651SQgD5K87LcjjD2Uwad429qR4gECu75jA5BvuIjI0EA61gfxUyFtlCkuQl1u1Wv1QCFGNFR0uOc9ZXMbPD4AroeR18U4+QJaziKcWbOfGt9exJyWbVuE5fDT0SV65rtCEOEDAxcU3ModzrU0uTpIWuRCifFyl9pvNXQJRz57685NdIp3NiJT8dWitWbI9mWcW7iAlqwCbRTHuihY83PaPOFybzbjuYvY2JeeWemBtUnnPUstIkAshzs1TYKbUYzV9486NZsMHW6kd6IuDvMH9kPYUiZkFPP3hGpbtzgKge3QOM/q9S4fQ78DlMWVLB3lAqSC3t69VMy8rmwS5EOLcirtMbM3Mrjp5yyBvacl+l1pDrglyt+NKPtw7jpfXx5HryiIs0MYTg4K4Peo6LEoDVrNgVdjNYCu13on94lLn0q1yPqSPXAgB+Rsg8Q5wnWE+X3H/eEBLCLnGnOcsKfl54R5wJ7PjRBzXv5/B1DVXkOsK5po2KSx9bAB3tJllQrz+n6BdNrTeAg0nn/oZ9t+0yEW5SYtcCGF2p8+ZB7bm0GjG6T93FQd5Kwi9BtK8O/ZoDygLeSdW8tqmu3lvxxjcOpPoejA1bipD2gEh10HyJ4CCiMfBEnT6/QEsEWCpb1ZElCA/L9IiF0JA0V5zzP7EdJOc9nPvF50BLc3aKLYWZhefgi2s3JPK0Pfr8872G/BoxV19Y1j6aB+GtNwIzp8g43UzrjzkWrOy4Zko5R3SGGB29hHlJi1yIeo67SmZ7FN00OysU7w5Q7HirhVbKxO4IdeQlvwZU2evZuG+NkAYHRv8wgs3D+CS1t4vMANjoWArHP+beR3x0LnrEv0RuNPALruFnQ8JciHqOlcCaGfJ66w5ZQR5SYvc49H8b/8oZiy9iszCMBxWJ491n809XTdiixlf8p6gPibIdaH5gjR48LnrYmt46kgYUS7StSJEXVe4zxyt3gDN+tS7Pngp3j7y/SeiueXd9UxYBJmFYVwRk8HSWz9mXLf12CLHnzpksPRfBvXHg5K4qSzSIheiriv09o+HjoS876FonxkTHjLEXNeaAmcib229lX9tS6TQrWkYauepEZ0YeUlTlDrDSodBl5ujJdSsDy4qjQS5EHVdcZDb25kvMdOfNd0r3iDfsP8XJn3xdw5ktgA0v4trwcThHagffI5lZu0XQ9NPwRYN1nqV+wx1nAS5EHVdcdeKva2ZXp/+LGR/zomw15ix5Fc+3XQEaMFF9dOYcfN1XHZRZPnvXe/mSqmyOJUEuRC1XdZcKNwJkU+W3U9dukUe2B5t78GXu8N47tNVHMtV2K2aP3WZwwN98whsdVeVVl2UjwS5ELVZ/kZIvAVwQ9AVEDLw1J/rIij6BVAQcDFHjucx+dsnWH0wFIBerSOYflU8bdwfg6McwweFX0iQC1FbefIg6Q7AbV7nfHl6kBcdBNy4LDG8/30i/1i2F2dRKOH2bCb1nM1Nwz7Fkv4pZAC2llVafVF+EuRC1FapfzVroFgbgvuYCfJGL586RLBwL5vT2jFx/UR2pe8GYFS3pjzZ9TGiLCshf0mpdVZaVf0ziHKRIBeiNsr5Bk68AQRAiyVw5Gqz8UPhLgjsZIoUuHhpcQYfxr+ExkLzBkE8PyaWAe2iIH04pK2ErM9OnZ4vqiUZoS9EbZT6sDlGTQXHpRAywrzO+RKAb3ckM+SVVXwQ3wCL0tzX6zjf/vkKE+IAYTd6yy/09qEjQV6NSZALUdu4s81IFBVoVhsECBsJQHLyd9z/UTzjPoonKdNJ16hkvhzxKBOHRhJsL/UPdHtrs3CVzjWrESo7WBuX8WGiOpCuFSFqm6JS48KV+V/cEzSE2btH8rf435NTlEyI3crjV7fnzqgHsXoOmbK/FXYzODeZc1sLmWJfjUmQC1HbFI8LD2gHwO7kLCbO28bPh8cBMLhNAVNvHEbTesDeQ4ANAmJOv0/YjZD2hPde8kVndVbuv2KVUrOUUqlKqe2lrj2jlDqqlNrs/TW8cqophCi3wj0AOFUH/v7Nbkb8cw0/Hz5Bo1A3bw2czrtX/4em9YNKlq61XwQq4PT7FHevgPSPV3Pn0yL/AHgD+M9vrv9Da/2Sz2okhKiYwr2sTbyEST/24tCJAygFd/Ruxf8Nqke9o+sgL8hs6VbcBRNQRrdKsfB7TfeKo1fV1F1ckHIHudZ6tVIqpvKqIoSoqOO5hUxb0ol5e28DoH3jMKZfH8ulrRqYAkF9IP8H+KWd2QAZzNT8M6l/HwT3B3uHSq65qAhffHsxXim11dv10uBMhZRS45RSm5RSm9LS0nzwsUKIYlpr5sYnMOjllczb241AawH/N6Q5Cx+6vCTEAaJnQ8jV4MmE/O/NtbK+6CymFAR2BmWt3AcQFVLRIH8LuBjoBiQBL5+poNZ6ptY6TmsdFxUVVcGPFaIOKGvvzDL8eiyX29/bwOOfbSEjr4h+0Zv5ZswkHryqK3bbb/4Xt7eG5ouh+VfeLhULBPX1fd1FlarQqBWtdUrxuVLqXeCrCtdIiLrKnQHHnoOCzVD4C7gSoeEUaDi5zOKFLg8zVx/gnyv2U+jy0CA4gCeHWLm+/pOooMtOnYpfmlIQeq1pmbuPg61RJT6UqAoVCnKlVLTWOsn7cgyw/WzlhRBnoF1w9GbIW3bq9Yw3IHLiaWO44w8dZ+K8bexNyQHghh7NmXxtRyKK/gPJnL3fu5iySYjXEuUOcqXUHGAg0FAplQBMAQYqpboBGjgI3FcJdRSi9kubYELc2giiZ5kgPjwEXIfA+SME9QYgM7+IF5fsZvYGs5BVTGQwz4+JpV8b736bqWboIfb2/ngK4SfnM2rl1jIuv+/DughRN2V+DMdfBmzQbK4ZJQJmWn3G65DzJdpxGV9vS+aZhTtIyy7AZlHcP+Bixl/VBkdAqS8iS28SIeoMmdkphD8V7IDke81543+WhDhA6CjIeJ2jSat4etEmlu9OBeDSVg2YPiaW9k3CTr+fBHmdJEEuhD9lvAnaCfXugPr3n/Ijt6M/H+z6HS/H30ieK5WwQBt/vboptzW5HYurLXhmg8VR8gbtgsID5tzepgofQvibBLkQ/qI9kLPAnEc8csook+1HM5k4bxvbjt4BwPD22Uy5YTSNc/4AWdugcJvZwq3Z3JMLY5ndforMAleWkKp9FuFXEuRC+ItzA7iSzBZqgT0AyCt08Y+le3l/za94NDSt52Fqz2kMbucwX4RmzQblABUEOV9A0j0Q/YEZ1SLdKnWWBLkQ/pI93xzDxoBSfLc7lScXbOfoiXwsCu7p15rHBjUm9PDPkO+GZLN6IZGTIWQIHB4EWR+BJdz0r0uQ11kS5EL4g9YngzxVjeLZj39i0VYzJaNz03q8cH1XYpuHm7LBAyFvqdmqzd4eIv4PLIHQfAEkXGu2dFNW09cOEuR1kAS5EP5QuANP4QE+2X8DL8QXkuVMIijAymND2nF3vxhs1lITgMJGmiAHaPwvE+IAIYOh2edw9AbIeM3sCAQS5HWQBLkQfrD/4GImLn6BjamdARcD20fx3KgutIgIPr1w2O/MDM+Q4RBy1ak/Cx0BzebB0etBF5hrMhmozpEgF6IKOYvc/GvlAd76rh1FHhsNQzRTRvZgRNdo1JnWRrFFwUW7z3zT0Guh2Xw4OgYsYbKbTx0kQS5EFfnhQDqT52/jl2O5gI1b2y1nwu+eJTykjIk95yt0OLTeBqiS4YiizpA/cSEqWUZuIdO/3sVn8QkAtIksYEavp+jZpgv4IsSLSd94nSVBLkRFFR2Bgm0Q1B+sJcGsteaLzYk899VO0nMLsVstjL+qDfe1vp/Awp0Q9qQfKy1qEwlyISpCe+DI1VC4C5QdggdBvds47BrD5AXb+X7fMQAuax3B9OtjuTiiAPatAmwQMsy/dRe1hgS5EBWRu8Qb4g7QBRRlf8t764N5bUs4TpeF8KAAJg/vyE1xzc2XmZnzATcEDwZrfX/XXtQSEuRCVETGP82x4bP8nHUjE+f9zO40s5DV6NggnhzVj4ahgSXlc74wx9CRVVxRUZtJkAtxoQp2Q+43ZBdF8NL3g/jPhl1o7aB5vTyev+wFBrTOheB4wBvkHifkLDHnYRLkwncquvmyEHVXxut8c6g3Q754hw/XJ2NRivsGXMTSx65hQGsnFO6BlEdLyud9BzoXArvJWG/hU9IiF+ICJB9PYcq8hnxz2Iw8uaR5ONOvj6VzU+/6KE0/gUM9IfM9cFwCDcaX6lYZ5adai9pKglyI8+D2aGZvOMSLi7eSU9iLkIBC/u+abtzRJwarpdTMTEcsNH4Dkv8IKQ+ZRbJyFpqfSbeK8DEJciHKaXdyFhM+38bmIycAK4NbrGfq6J40bda67DfU/4NZkTDlIUh92FyztYDA7lVWZ1E3SJALUVrhQUj+AwR2gkavgVI4i9y8tnwf767+BZdH0zi0kGd7/p1r2uVC02fOfr8G480x5SFzDB15yk5AQviCBLmoHbK/hJT7odmXEBR3YffI3wQJI8CdAnnLIbA736eNYPL87Rw+nodScGev+jze9jrqBWRA4+/NOuDn0mC8mSyU8To0+NOF1U2Isyh3kCulZgEjgFStdRfvtQjgUyAGOAjcrLXO8H01hTiHrP+abdOy5lxYkGcvNHtg6jywtyc9K4lpn+1h/oFGAHRoEsb062PpYRkLORlms+Tgy8t///rjzC8hKsH5DD/8ALjmN9cmAMu11m2B5d7XQlS9gu3e4+bzf2/RYbM5g85Dh43ls+NLGDR/FvMP9CfQWsQTV7dj4UOX0yNinfnC0hIGjf7m2/oLUQHlbpFrrVcrpWJ+c3kUMNB7/iGwEvirD+olRPl5CqBwnzl3bjYjRM6nHzpvNVDEL4U3MXnR/fzwyw7AQf+m25nW+1VaNb0MjqSAM96Ub/gs2KJ9/RRCXLCK9pE31lonAWitk5RSjXxQJyHOT+FewGXOPcfBlQABLcr/9txNvLPld7y+9XYK3elEhNh5akRHRl9ciEpMLhn/jYLQ0SVfYApRTVTZl51KqXHAOICWLVtW1ceKuqBwx6mvnZvLHeSbDh5n4qfd2JcRBcCNlzZn8vCONAixA83B8x64jkJQb3BcBtZwH1deiIqraJCnKKWiva3xaCD1TAW11jOBmQBxcXG6gp8rRImC4iC3Am4o+BnCrjvrWzLzi/jbkt18vKWn7SkAABwlSURBVOEwEEXrekd5/sbB9G130akF699bGTUWwqcqutbKl8BY7/lY4IuzlBXiwmhtfp1J8RedIVebo/PMX3hqrflqayKDX1nFxxsOE2CBh7p+wuIbXjo9xIWoIc5n+OEczBebDZVSCcAU4AXgf0qpe4HDwE2VUUlRB2XNhcx3zYiSosNm7e6Yn8DW+PSyxS3y8N9D7tdnHLmSkJHH01/sYMVu8w/HS1s1YMbgfbRz/xdCb66sJxGi0p3PqJVbz/CjQT6qixAlUh81fdPFXHmQPR8a3H9qOU8+FO0HrN5Zk4FQ9Cu4T5zcuMHl9vDBuoO8/O1e8ovchDlsTBjWgVt7tsSS+l84ATgucBKRENWALGMrqh93lglxFQgxW6DRq+Z67pLTyxbuBjTY24IlBAJjzfWCrQBsS8hk9L/WMm3RLvKL3FzbNZrljw3g9staYbEocG405SXIRQ0mU/RF9VO41xztbcHRFawNTAs9bznoQjPdvVhx/3hgZ++xGzg3kZu1hVdWNOTfa3/Fo6FZ/SCeG92ZqzqU6prxFIBzizl3XFr5zyVEJZEgF9VP4R5ztHcwx4AWYO9shhnmrYOQgSVli/vH7V3M0dGNFdvjeOrzxhzN/hWLgj9c3po/D2lHSOBv/nMv2AYUgb09WOtV5hMJUamka0VUPyeDvH3JtVDv6hC5i08tW6pFnprl5MGFbbln+TMczQ4htlk4X46/nCdHdCLElgNpz8D+VpDunV7v3GSOjp6V9ihCVAVpkYvqp6wgDxkGx1/29pOXWuekcAcerfh4a0v+tmwV2U4XwbZ8Hus+h7tGzcZmccGxaea9nhPmPWkTIKB1qSCX/nFRs0mQi+qnrCAPuhxUsPkSsygRApqCJ4e9qR4m/vAi8anHALiqQyOmdhtPc8ePkPE8ZM4CV6K5R/BA04ee8Sok3WX63kGCXNR4EuSietGeUl92lgpySyAEXwW5X0HuEpwhY3nz2428vfY1ijwBRIUF8sx1nRke2wSV2Aqyf4T0aea9jp4Q9TcIudJMLPJkQua/wZUPWMAhO/aImk2CXFQvrgTQ+WBtfPq6JqHXQO5XrNu9mclrLuLXY3lAALd12ctfbxhPeFCAKRfUB7I/MysURr0A9X4Pyvt1kFLQ+C0zbDH/BzPaxRJcpY8ohK9JkIvqpaxuFa8MNYTn1zzC3P1DgFzaNkhhRu+XiOt8OxSHOECDB01AB/UFS+jpn2EJhGbzzPZrYTKjU9R8EuSieinYbY6lglxrzfyfjzJt0SGO5w7Bbink4Us+YVyXeditQMhVp95D2SFk6Nk/x9YEmn3m27oL4ScS5KJ6+U2L/FB6LpPnb2fNfvNlZp9WRTzf5x0uahIDwR+ZLzBtTfxTVyGqCQlyUb14g7zI1p6Z3+3nn8v3UeDyUD84gMnDO3Ljpc1RarSfKylE9SJBLqqXwj38lNqeSYsD2Z1iQn1M92Y8eW1HIkMD/Vw5IaonCXLhX7nfmiGHodeQnZfJ39dcy0e7h6MppGVEMM+P6UL/tlH+rqUQ1ZoEufAfTx4kjARdxJL81TzzdRbJ2SOwKjfjBlzMw1e1Jchu9Xcthaj2JMiF/xTsICknlCkb7ufbw2b6fLeGu5kxeAcdu83yc+WEqDlk0Sxx/ooOQeJtkL/pgm/h9mg+WLufwfPf4tvDfQgNyGNq37l8PvwJOjZt5MPKClH7SYtcnL+MtyFrDuR+B61/Pu/hfzsTs5g4fxtbjpilY69uvYdn46bTJCTdFChjMpAQ4sykRS7On3O9ObqTTctcu8r1tvxCNzMW7+K6N9aw5cgJmoRk886V03jn9g40adSnpKAEuRDnRYJcnJn7uGl5a3fJNe2GfO/2aNZIyPsOjk05561W7U1j6KureGfVL3i0ZmyfViwd8xhXt1oPgZdA49fN6obKDoEdK+mBhKidpGtFnFnaU3DiX9A4s2TT44IdoHMhIAaazIIjgyF9OgT1g9Dhp93iWE4Bzy3cxhdbUgDo0CSMGdfH0j06Fw4kgSUCbE3NYlatvgdPdsnyskKIcpEWuTiz/DXmmLOw5Jpzgzk6eptlYRs+Z14njQVX0sliWmv+t/EIg15exRdbUnBYnfw17mMWPtiT7i0bnNwcGUdXE+IAjh4QPKCSH0qI2kda5KJsnvyS/TDzvgOPEywOyPf2jwddZo6RE8zP85ZB4lhosYQDx/KYNG8bG349DkD/pj/xfJ83aRmWAoV3QcCQkiAPvKRqn0uIWsgnQa6UOghkA27ApbWWLVdquoKtmD9OzPrg+d9DyBDI97bIi4NcWaDpf+DXrhRkf8fbX/2HN9c3ptDtITIkgKd6/YdRLWejbI3M7XK+Mvc5GeRdq/rJhKh1fNkiv1JrfcyH9xP+5Pzp1Nc5S8BxGRTuBAIgsNSuOrZofiz6NxO/TOBAZhTg4eZuVib1WUH9/NkQ0BaavA1HBplumkavglOCXAhfka4VYbpNXEfBfnHJNWe8OYaOgZz5ZtPj0OGABkc3080CZOYV8cKSXcz5UQEtuKheAs/3eZM+0dsg33uv6PfNJg/WhlD0KxRsMTv0YIHATlX4oELUTr4Kcg18q5TSwDta65m/LaCUGgeMA2jZsqWPPlZUmCvNtJQLdkBMvAlpKAnyBg+Z/u/CnZA911xzXIbWmoVbk5i6cCfHcgoIsCoeGBDDA5fsIdAdC0XhZtPj8LEQ3N+8L2Q4ZP0Hjr8IuM14cdlmTYgK81WQ99NaJyqlGgFLlVK7tdarSxfwhvtMgLi4OO2jzxUV4UqDI1dBwXbzOvMjE+Qep/eagqCeEDwIchbAifcBOFLQl6c+2MjKPWkA9IxpwIzrY2nTKAw4Sws7dIQJ8qxPzWvpVhHCJ3wy/FBrneg9pgLzgV6+uK+oRCdb4tvB6p1in/2Z2WW+YBvgAnsHs+dlyDXmLR43M7ePYeh74azck0Y9h40Z18fy6bg+3hA/h5ChmLaDx7yWIBfCJyrcIldKhQAWrXW293woMLXCNROVK+luE9j2DtByBRzsCa4jZpy4c7Mp47jUHEOuZuuxNkxcN54dx9sAmhFdo3n6uk40CnOU/zOt4WaceN5y81qCXAif8EXXSmNgvjKTOmzAx1rrJT64r6gs2gV5K8x5i2/BFg1hN0LGa5D1GXiyzM8cl5JT4OLlb3P5cN0reLSFZmFZTLtxEFe2v8AVCkNHSJAL4WMVDnKt9S+AzOqoSQp3m7HhAa0hoIW5FnaTCfLsz8waKsCyQ115eskqEjOdWJXij53n8efB7QiOrsAys6EjIPXPZmp+QCsfPIwQQoYf1kXFI1IcPUquBfUBWzNwHSElK5dnNkxk8SEzfjC2WTgzxrShS1g6hP2uYp9tbwPNPjd/WRRPzRdCVIgEeV1UPNmnuA8cQFnwhN7I7PV7eTH+LrKLQgi2W/nL0Pbc1TcGq0UBY33z+WHX++Y+QghAgrxuKg7ywJIW+Z7kbCbNvYb4hCEADIo5ytRb7qBZ/SB/1FAIcR4kyOsa7Qbnz+bc0QNnkZvXV+zjnVW/4PJoooIyefayfzGs5w0oCXEhagQJ8rqmcJ9ZT9zWgrUHFZPnr+Zgeh4At1/Wkid6ryE8LxHCxvi5okKI8pIgry10kTmqgLOXc8Zz3FmP5zf/mc93m5UM2zYKZcb1scTFRACxwJ8qtapCCN+SIK8N3Fnwawewd4GW356xmNaaeT8lMm3lW2QUhGO3WXj4qjaMu+Ji7DbZY0SImkqCvDbI/8HszuNKMv3fDu8Ss1pD8n2gczgY8DaTv9jJ2v1mLZS+MfD8jVfQumGI/+othPAJaYZVR1qbafLl3J0e548l5ydmlZznraLw+L95c42Lq19bzdr96TQIzOaly19h9r09JMSFqCUkyKuj7HlwsHu5dqcHIL9UkGfNNqsXAvE7P+K6ha/y95/GUuBSXN/VwbIx93Fjh92ogOhKqLgQwh+ka6U6yl9ljpn/hoZTQVnPXFZrcG4057am4Eok69gCXvy+KbM3jkZjoVW940zr/Qr9Y/KhMAsCr6j8ZxBCVBkJ8uqoYKc5upIg73sIGXjmsq4j4E4BSwQ6YiLfbPyYp/+nSM3LxqY83Bf3Cw8N7Y/j6BYo9C4DX3pqvhCixpMgr46Kd68HyP7k7EHu7VZJdF3J01/2YNnu1gD0iNrN9L5v0OHSb8HeFsL/AJnvmveUnpovhKjxpI+8unEfB3cyJ/9osueWjBEvq3jej8zaOZIhc+5k2e4MwuyFPNf7TeYO/z86tOhhQhwgahpY6gE2s+uPEKLWkBZ5dVPcGnd0B0++2SszdzmEXnNa0R2JmUz6pB1bUgcAMKxLE54ZfJzGJxabApGPlxS2NYJWa8GdbtYfF0LUGhLk1U1x/7i9s9nV/tgUyPrklCDPK3Tx6rJ9vL/mV9yeaKKD05g6pg9DYtuD9oDn92AJN0vTlhbYpQofRAhRVSTIq5tCb4s8sDOEjjJBnjMfPG+DxcHKPak8uWA7CRn5KAV3dfySx3t+R2inveZ9ygJNP/Jf/YUQVU6CvLopKBXkge0hsDsU/ExayhKmrmzFwi2JAHSKrseMoYe4RM0027QJIeosCfLqpnTXCuAJu4X/bYti+qYCsgoTcQRY+PPgdtx7eWtsaXPgBOCQLy+FqMskyKuT4hErKhgCWrI/NZtJ8/ry40ET6gOabWJan3/TovkIcD9cMjXf0cuPlRZC+JsEeXXi7VYpsMXyr2X7+dfK/RS5NQ1D7Tw11MHI6J9QuYfgxJvmF8r8knHhQtRpEuSVxZMD7hMQ0PzMZfJ/gGPPQeNXwd4OCnawPrkLkzY8wi8Z+wC4pWcLJgzrQP1gO9AfnNvMbvdZ/wVdAIGXgDWsap5JCFEtSZBXlsSxkPs1tN5aMinnt1InmnVVUjyciPySGYsUn25/AYCLo0KYPiaWyy6KPPU9jliIfg+iXoCceeDoU8aNhRB1iU+CXCl1DfAaYAXe01q/4Iv71lhaQ94y0E7I+Qoi/nx6mcJfIX8VWsOX25w8F7+MY7nNsVuKeODyAP40tD+BtrMslmVrCPXHVd4zCCFqjAoHuVLKCrwJDAESgI1KqS+11jsreu8aq+gQeLLMee7ysoM86z8cyW7M5PUPsfpoN0DTq8kepvf+B20uXQNnC3EhhCjFFy3yXsB+rfUvAEqpT4BRQN0N8oItJef5q8xaKaX20ixyuZm1OoF/xL+J0+0g3J7DpLj3uantMiyWYLC18EOlhRA1lS+CvBlwpNTrBOCy3xZSSo0DxgG0bNnSBx9bjRVsLTn35ED+RgjuC8CWIyeYMPcHdqWMBmDkJdE81e87ovKWmvKBnczsTCGEKCdfBLkq45o+7YLWM4GZAHFxcaf9vNpzbgFLGNgvOnfZ4ha5NQrcaZC3nBxrL176Zg8f/nAQraF5aDLThhxj4GXXgqcDHHjJjCEP7FypjyGEqH180fRLAEr3BTQHEn1w3+rDnQ6H+sCRweaLzHNxelvk9R8AYOn2gwx5ZRUfrDuIRSnui13It6MeZGC3UaacJRga/R2wQMiIynkGIUSt5YsW+UagrVKqNXAUuAW4zQf3rT6cP4POh6JfoWj/mYcTAnhyTRkCSLbeyzPfFbHkUF/AySXNw5k+NIHOnnfMbMzADiXvC/891LsZlL2yn0YIUctUOMi11i6l1HjgG8zww1la6x3neFvNUrCt5Dxv7dmDvGA7bo9i9oG7eXHOTnIK+hJiy+Pxq0K5s38nrIfvAA8Qftfp75UQF0JcAJ+MI9dafw187Yt7VUulgzx/DdS/64xFdx/ZwcTFL/JzWgfAxeCL05na4y80bXEXJP8Lig6aKfXh91RypYUQdYXM7CyPU4J8bZlFnEVu/rl8HzNXR+HyNKZRSBFTx1zG1THbUAnHIOOfoAvB0gCazgVLYBVVXghR20mQn4t2l6wRruxQuBtcaWCLOllkzb5jTF6wjUPpeSgUv2+/iCdGjqJeZDR4ws37dKEp3PQjsMdU/XMIIWotCfJzKfrFfNFpa2b6xvNWQv46CBtFek4Bzy/axbyfjwLQvnEo03v+hUsbxkP4FPN+SzAEDYC8pRA5CUKv9d+zCCFqJQnycykeShjYFRw9IG8lOm8tn++9lOcX7SQjr4hAm4WHB7Xlj5dZsB+OB2sjsDUuuUf0u2alw7Cb/PMMQohaTYL8XIr7xwNjIehyfs1qyuSlLVl31Ez66dcmkudHxxLTMASyv/SWveTUewS0Mr+EEKISSJCfizfIC22xzFzfnH+ueINCj50GwQE8eW0nru/RDKW8k1uLZ3Q6uvqpskKIukiC/FwKthGf2oGJi6LYm3YIsHPDxcuYPGY0EQ2bmwWxCg+COwPyVpn3/LZFLoQQlUiC/CwyczN5cfUQZu8ZDriIiQzm+QHf0a/eq6AaQ3YGpDwEroRT3yhBLoSoQhLkZdBas3h7Ms98sZnUnOHYlJv7B7Zn/FVtcOQlQRJwbBroXPMGW1OwRZsx4o44058uhBBVRIL8N46eyOfpBdtZvjsVgB5Ru5gxZD/tu440BYIvN0eda1ZDjJoO9f8ESjaCEEL4hwS5l9uj+WDdQV7+dg95hW7CAm38td82bmvxBJaoaSUFA1pC5EQzKajhMxDQzG91FkIIkCAHYPvRTCbO28a2o5kADI9twpTrOtP4xN8hT5/eVRI13Q+1FEKIstXpIM8rdPGPpXuZtfYgbo+mabiDqaO6MLiTdzJPSqkx5EIIUU3V2SD/bncqTy7YztET+ViU5p4+Efzlml6EBHp/S1zJ4E41/eAymUcIUY3VrSDXmtScAqYu3MlXW5MA6NzYxYyej9O1eQMI/LGkbPZccwzqC6qs3eyEEKJ6qDNB7slZxqcrXmHGTw+Q5VQEBVh5bEg77o75P2x5+8EJOOPNWuEAJ943x/C7/VZnIYQoj9oT5NoFquzH2Z+azcRPEtmY+CAAA9tH8dyoLrSob4F9S0oKZrwF0e+B8yco2AyWCAgdXRW1F0KIC+aLzZf9y1MAibfBvoaQ9/0pP3IWuXll6V6GvfY9GxMjaejI4PUBf+PftwbRIiIY8paDzgNbc/OGrDngPlGqNX6HbAAhhKj2anaL3JMHR2+AXG+rOukuaL0VLCH8cCCdyfO38csxM/vy1nZLmXDpe4QH5kLWx+CYATne1Qrr/9Gsk5K3Ak7MhKzZ5nr4vVX/TEIIcZ5qbpC7syHhOshfBdaG5lfhbjIOT2H6xjv4LN6sf3JxVAgzrgujl+c1IMC8N+tjiJpWsuxs6EiwdzRBnvYkUASOnuCQYYdCiOqv5gZ50l0mxG3R0GI52uPkixWP89zG7qQ7E7BbLYy/qg33DbiIwNyPzfooodea/m/XYTj+D3Ang62lWeQqsDNYm5hrIK1xIUSNUTP7yAt2QM48UA5ouZrDOa248+NCHv3+MdKd9bkseh+LH47j4UFtCbRZwfmzeZ/jUgi/3Zwfe9IcQ0ea4YUqAOr/wVxTwVDv1qp/LiGEuAA1s0V+/CUAikLv5b11iteWr8JZ5CE8yMbkXp9wU8wHqAAr8JApX1Ac5N0hoDWkzwBdYK6FjSy5b4MHIWchhN0I1npV9zxCCFEBFQpypdQzwB+BNO+lSVrrrytaqbMqSoDM2WxOa8+ExaPZnbIbgFHdmvLUiE409ByHxA/MhJ6Ih0DrkhZ5YHcIaAqB3bzDC+tB8ICSe9uaQOvNlVp9IYTwNV+0yP+htX7JB/cpl+zk13l5w918uOs6NAW0iAhi2uhYBrSLMgXc14IKhPzvzTR7Tx54Mr0bIkebMuF3QupmCB0Byl5VVRdCiEpRo7pWvtl2gCnzO5OcF4lVwR+uuIhHB7UjyF5qLXBrPQi52gwtzJ5vAhxMt0rxVPsGD4El3AS5EELUcL74snO8UmqrUmqWUqrBmQoppcYppTYppTalpaWdqdgZ5RS4mDhvO8l5kVzSKImFD/Vn4rCOp4Z4sbAbzTF7bqn+8R6lKmOD+veArdF510MIIaobpbU+ewGllgFNyvjRZGA9cAzQwHNAtNb6nnN9aFxcnN60adP51dTj5KulN5GeB78fPB5r2NVnLus+AfsaAW4zUsW5EZr+D+rddH6fKYQQ1YhSKl5rHffb6+fsWtFaDy7nB7wLfHUBdSsfZWdE3z9CzhcQOvTsZa31IWQI5H5tQhxM14oQQtRCFepaUUpFl3o5Btheseqc7cMsZqhg9PvlW1a2uHsFvGuKX1RpVRNCCH+q6JedLyqlumG6Vg4C91W4Rr4SNgqSbYDLDDdUNXPukxBCnEuFglxrfYevKuJz1ggIvhLylkq3ihCiVqtRww/PW8PJkJIim0MIIWq12h3kwQOg9RZ/10IIISqVdBwLIUQNJ0EuhBA1nAS5EELUcBLkQghRw0mQCyFEDSdBLoQQNZwEuRBC1HAS5EIIUcOdcxnbSvlQpdKAQxf49oaYpXPrEnnmukGeuW6oyDO30lpH/faiX4K8IpRSm8paj7c2k2euG+SZ64bKeGbpWhFCiBpOglwIIWq4mhjkM/1dAT+QZ64b5JnrBp8/c43rIxdCCHGqmtgiF0IIUYoEuRBC1HA1KsiVUtcopfYopfYrpSb4uz6VTSnVQin1nVJql1Jqh1LqEX/XqSoopaxKqZ+VUl/5uy5VQSlVXyk1Vym12/tn3cffdapsSqk/e/+b3q6UmqOUcvi7Tr6mlJqllEpVSm0vdS1CKbVUKbXPe2zgi8+qMUGulLICbwLDgE7ArUqpTv6tVaVzAX/RWncEegMP1oFnBngE2OXvSlSh14AlWusOwCXU8mdXSjUDHgbitNZdACtwi39rVSk+AK75zbUJwHKtdVtgufd1hdWYIAd6Afu11r9orQuBT4BRfq5TpdJaJ2mtf/KeZ2P+B2/m31pVLqVUc+Ba4D1/16UqKKXqAVcA7wNorQu11if8W6sqYQOClFI2IBhI9HN9fE5rvRo4/pvLo4APvecfAqN98Vk1KcibAUdKvU6glodaaUqpGKA7sMG/Nal0rwJPAB5/V6SKXASkAf/2die9p5QK8XelKpPW+ijwEnAYSAIytdbf+rdWVaax1joJTEMNaOSLm9akIFdlXKsTYyeVUqHA58CjWussf9ensiilRgCpWut4f9elCtmAHsBbWuvuQC4++ud2deXtFx4FtAaaAiFKqd/7t1Y1W00K8gSgRanXzamF/xz7LaVUACbEZ2ut5/m7PpWsHzBSKXUQ03V2lVLqv/6tUqVLABK01sX/0pqLCfbabDDwq9Y6TWtdBMwD+vq5TlUlRSkVDeA9pvripjUpyDcCbZVSrZVSdsyXI1/6uU6VSimlMH2nu7TWr/i7PpVNaz1Ra91cax2D+fNdobWu1S01rXUycEQp1d57aRCw049VqgqHgd5KqWDvf+ODqOVf8JbyJTDWez4W+MIXN7X54iZVQWvtUkqNB77BfMs9S2u9w8/Vqmz9gDuAbUqpzd5rk7TWX/uxTsL3HgJmexsovwB3+7k+lUprvUEpNRf4CTMy62dq4VR9pdQcYCDQUCmVAEwBXgD+p5S6F/MX2k0++SyZoi+EEDVbTepaEUIIUQYJciGEqOEkyIUQooaTIBdCiBpOglwIIWo4CXIhhKjhJMiFEKKG+3/um/p3YxLtvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(T,X,label='Actual',lw=2,color='gold')\n",
    "plt.plot(T,predicted,label='Model',lw=2)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
